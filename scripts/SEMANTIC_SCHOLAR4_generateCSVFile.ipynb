{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971bc53a-1e07-45c4-a813-f26d0c153c6d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarxiv in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (1.0.3.1)\n",
      "Requirement already satisfied: feedparser in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from pyarxiv) (6.0.10)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\z213165\\appdata\\roaming\\python\\python310\\site-packages (from pyarxiv) (2.8.2)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from feedparser->pyarxiv) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\z213165\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil->pyarxiv) (1.16.0)\n",
      "Requirement already satisfied: pybtex in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: six in c:\\users\\z213165\\appdata\\roaming\\python\\python310\\site-packages (from pybtex) (1.16.0)\n",
      "Requirement already satisfied: latexcodec>=1.0.4 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from pybtex) (2.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.01 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from pybtex) (6.0)\n",
      "Requirement already satisfied: dropbox in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (11.36.2)\n",
      "Requirement already satisfied: requests>=2.16.2 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from dropbox) (2.28.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\z213165\\appdata\\roaming\\python\\python310\\site-packages (from dropbox) (1.16.0)\n",
      "Requirement already satisfied: stone>=2 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from dropbox) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.16.2->dropbox) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.16.2->dropbox) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.16.2->dropbox) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.16.2->dropbox) (2022.12.7)\n",
      "Requirement already satisfied: ply>=3.4 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from stone>=2->dropbox) (3.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pycolors2 in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (from pdf2image) (9.4.0)\n",
      "Requirement already satisfied: Unidecode in c:\\users\\z213165\\appdata\\local\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyarxiv\n",
    "!pip3 install pybtex\n",
    "!pip3 install dropbox\n",
    "!pip3 install tqdm\n",
    "!pip3 install pycolors2\n",
    "!pip3 install pdf2image\n",
    "!pip3 install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4d4a1a-89e6-4f70-bc4e-77cf301d9191",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd \n",
    "import string\n",
    "from bib_handling_code.processbib import read_bibfile\n",
    "from bib_handling_code.processbib import save_to_file\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e5d466-32a0-4522-8675-8ce625b4cfcc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "staff_id_dict = {'Bram van Ginneken': [8038506, 123637526],\n",
    "'Francesco Ciompi': [143613202],\n",
    "'Alessa Hering': [153744566],\n",
    "'Henkjan Huisman': [34754023],\n",
    "'Colin Jacobs': [2895994],\n",
    "'Peter Koopmans': [34726383],\n",
    "'Jeroen van der Laak': [145441238, 145388932],\n",
    "'Geert Litjens': [145959882],\n",
    "'James Meakin': [4960344],\n",
    "'Keelin Murphy': [35730362],\n",
    "'Ajay Patel': [2109170880, 2116215861],\n",
    "'Cornelia Schaefer-Prokop': [1419819133, 1445069528, 1400632685],\n",
    "'Matthieu Rutten': [2074975080, 2156546],\n",
    "'Jos Thannhauser': [5752941],\n",
    "\"Bram Platel\" : [1798137], \n",
    "\"Nico Karssemeijer\" : [1745574], \n",
    "\"Clarisa Sanchez\" : [144085811, 32187701], \n",
    "\"Nikolas Lessman\" : [2913408], \n",
    "\"Jonas Teuwen\" : [32649341, 119024451], \n",
    "\"Rashindra Manniesing\" : [2657081],\n",
    "\"Nadieh Khalili\": [144870959]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d95c791e-7ebb-489d-8e58-981b93864d7c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "staff_year_dict = {\n",
    "'Bram van Ginneken':  {'start' : 1996, 'end': 9999},\n",
    "'Francesco Ciompi':  {'start' : 2013, 'end': 9999},\n",
    "'Alessa Hering':  {'start' : 2018, 'end': 9999},\n",
    "'Henkjan Huisman':  {'start' : 1992, 'end': 9999},\n",
    "'Colin Jacobs':  {'start' : 2010, 'end': 9999},\n",
    "'Peter Koopmans':  {'start' : 2022, 'end': 9999},\n",
    "'Jeroen van der Laak':  {'start' : 1991, 'end': 9999},\n",
    "'Geert Litjens':  {'start' : 2016, 'end': 9999},\n",
    "'James Meakin':  {'start' : 2017, 'end': 9999},\n",
    "'Keelin Murphy':  {'start' : 2018, 'end': 9999},\n",
    "'Ajay Patel':  {'start' : 2015, 'end': 9999},\n",
    "'Cornelia Schaefer-Prokop':  {'start' : 2010, 'end': 9999},\n",
    "'Matthieu Rutten':  {'start' : 2019, 'end': 9999},\n",
    "'Jos Thannhauser': {'start' : 2022, 'end': 9999},\n",
    "\"Bram Platel\" : {'start' : 2010,  'end' : 2019},\n",
    "\"Nico Karssemeijer\" : {'start' : 1989, 'end' : 2022}, \n",
    "\"Clarisa Sanchez\" : {'start' : 2008, 'end' : 2021}, \n",
    "\"Nikolas Lessman\" : {'start' : 2019, 'end' : 2022}, \n",
    "\"Jonas Teuwen\" : {'start' : 2017, 'end' : 2020}, \n",
    "\"Rashindra Manniesing\" : {'start' : 2010, 'end' : 2021},\n",
    "\"Nadieh Khalili\" : : {'start' : 2023, 'end' : 9999}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_blacklist_items(df_new_items):\n",
    "    blacklisted_items = pd.read_csv(\"./script_data/blacklist.csv\")\n",
    "    initial_length = len(df_new_items)\n",
    "    df_new_items = df_new_items[~df_new_items['ss_id'].isin(blacklisted_items['ss_id'].unique().tolist())] # remove blacklisted dois\n",
    "    df_new_items = df_new_items[~df_new_items['ss_doi'].isin(blacklisted_items['doi'].unique().tolist()) | df_new_items['ss_doi'].isna()] # remove blacklisted dois\n",
    "\n",
    "    print(f\"{initial_length-len(df_new_items)} items removed from newly found items.\")\n",
    "    return df_new_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae2a87c-359a-47a2-b7bf-9848b5afadd1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def from_bib_to_csv(diag_bib_raw):\n",
    "    bib_data = []\n",
    "    bib_columns = ['bibkey', 'type', 'title', 'authors', 'doi', 'gs_citations', 'journal', 'year', 'all_ss_ids', 'pmid']\n",
    "    \n",
    "    for bib_entry in diag_bib_raw:\n",
    "        if bib_entry.type == 'string':\n",
    "            continue\n",
    "\n",
    "        bibkey = bib_entry.key\n",
    "        bib_type = bib_entry.type\n",
    "        fields = bib_entry.fields\n",
    "        \n",
    "        bib_authors = fields.get('author', '').strip('{}')\n",
    "        bib_title = fields.get('title', '').strip('{}')\n",
    "        bib_doi = fields.get('doi', '').strip('{}')\n",
    "        bib_gscites = fields.get('gscites', '').strip('{}')\n",
    "        bib_journal = fields.get('journal', '').strip('{}')\n",
    "        bib_year = fields.get('year', '').strip('{}')\n",
    "        bib_all_ss_ids = fields.get('all_ss_ids', '').strip('{}')\n",
    "        bib_pmid = fields.get('pmid', '').strip('{}')\n",
    "        \n",
    "        bib_data.append([bibkey, bib_type, bib_title, bib_authors, bib_doi, bib_gscites, bib_journal, bib_year, bib_all_ss_ids, bib_pmid])\n",
    "\n",
    "    df_bib_data = pd.DataFrame(bib_data, columns=bib_columns)\n",
    "    return df_bib_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721fb2ba-0a2c-491b-847c-83aecbacc0d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_new_ssids():\n",
    "    staff_dict = {key: {'ids': staff_id_dict[key], 'years': staff_year_dict[key]} for key in staff_id_dict}\n",
    "    all_staff_id_ss_data = []\n",
    "\n",
    "    for idx, (staff_name, values) in enumerate(staff_dict.items()):\n",
    "        staff_ids = values['ids']\n",
    "        staff_start = values['years']['start']\n",
    "        staff_end = values['years']['end']\n",
    "        print(f'[{idx + 1}/{len(staff_id_dict)}]: {staff_name}')\n",
    "\n",
    "        for staff_id in staff_ids:\n",
    "            print('\\t\\t', staff_id)\n",
    "            staff_id_ss_data = []\n",
    "\n",
    "            url = f'https://api.semanticscholar.org/graph/v1/author/{staff_id}/papers?fields=year,title,authors,externalIds,citationCount,publicationTypes,journal&limit=500'\n",
    "            r = requests.get(url)\n",
    "            ss_staff_data = r.json().get('data', [])\n",
    "\n",
    "            for ss_staff_entry in ss_staff_data:\n",
    "                ss_id = ss_staff_entry.get('paperId')\n",
    "                ss_title = ss_staff_entry.get('title')\n",
    "                ss_doi = ss_staff_entry['externalIds'].get('DOI')\n",
    "                ss_citations = ss_staff_entry.get('citationCount')\n",
    "                ss_year = ss_staff_entry.get('year')\n",
    "                pmid = ss_staff_entry['externalIds'].get('PubMed')\n",
    "                authors = ' and '.join([author['name'] for author in ss_staff_entry.get('authors', [])])\n",
    "                ss_journal = ss_staff_entry['journal'].get('name') if ss_staff_entry['journal'] and 'name' in ss_staff_entry['journal'] else None\n",
    "                \n",
    "                        \n",
    "                if ss_year != None:\n",
    "                    ss_year = int(ss_year)\n",
    "                    if not staff_start <= ss_year <= staff_end:\n",
    "                    # probably doesnt belong to DIAG, still captured via another staff member if also in the same paper\n",
    "                        continue\n",
    "                staff_id_ss_data.append([staff_id, staff_name, staff_start, staff_end, ss_year, ss_id, ss_title, ss_doi, ss_citations, pmid, authors, ss_journal])\n",
    "                \n",
    "            all_staff_id_ss_data.extend(staff_id_ss_data)\n",
    "\n",
    "    ss_columns = ['staff_id', 'staff_name', 'staff_from', 'staff_till', 'ss_year', 'ss_id', 'title', 'doi', 'ss_citations', 'pmid', 'authors', 'journal']\n",
    "    df_all_staff_id_ss_data = pd.DataFrame(all_staff_id_ss_data, columns=ss_columns)\n",
    "\n",
    "    print('DONE')\n",
    "    return df_all_staff_id_ss_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45e1142-5055-4d00-a355-30c613ce4b68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def return_existing_ssids(bib_file):\n",
    "    all_ss_ids=[]\n",
    "    for entry in bib_file:\n",
    "        if entry.type == 'string':\n",
    "            continue\n",
    "        if 'all_ss_ids' in entry.fields:\n",
    "            ss_ids = entry.fields['all_ss_ids'].translate(str.maketrans('', '', string.punctuation)).split(' ')\n",
    "            if len(ss_ids) > 1:\n",
    "                all_ss_ids.extend(ss_ids)\n",
    "            else:\n",
    "                all_ss_ids.append(ss_ids[0])\n",
    "    return all_ss_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea4d1227-a735-47e5-975e-ae4cb703a691",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_diag_bib = os.path.join('script_data/', 'diag_ss.bib')\n",
    "diag_bib_raw = read_bibfile(None, path_diag_bib) # I changed the code in such a way that IF I give a second argument, it uses the second argument as a full path\n",
    "#Open the bibfile as pandas dataframe\n",
    "df_bib = from_bib_to_csv(diag_bib_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e9ab41-c4ec-4e17-9422-c283e2ac713a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20]: Bram van Ginneken\n",
      "\t\t 8038506\n",
      "\t\t 123637526\n",
      "[2/20]: Francesco Ciompi\n",
      "\t\t 143613202\n",
      "[3/20]: Alessa Hering\n",
      "\t\t 153744566\n",
      "[4/20]: Henkjan Huisman\n",
      "\t\t 34754023\n",
      "[5/20]: Colin Jacobs\n",
      "\t\t 2895994\n",
      "[6/20]: Peter Koopmans\n",
      "\t\t 34726383\n",
      "[7/20]: Jeroen van der Laak\n",
      "\t\t 145441238\n",
      "\t\t 145388932\n",
      "[8/20]: Geert Litjens\n",
      "\t\t 145959882\n",
      "[9/20]: James Meakin\n",
      "\t\t 4960344\n",
      "[10/20]: Keelin Murphy\n",
      "\t\t 35730362\n",
      "[11/20]: Ajay Patel\n",
      "\t\t 2109170880\n",
      "\t\t 2116215861\n",
      "[12/20]: Cornelia Schaefer-Prokop\n",
      "\t\t 1419819133\n",
      "\t\t 1445069528\n",
      "\t\t 1400632685\n",
      "[13/20]: Matthieu Rutten\n",
      "\t\t 2074975080\n",
      "\t\t 2156546\n",
      "[14/20]: Jos Thannhauser\n",
      "\t\t 5752941\n",
      "[15/20]: Bram Platel\n",
      "\t\t 1798137\n",
      "[16/20]: Nico Karssemeijer\n",
      "\t\t 1745574\n",
      "[17/20]: Clarisa Sanchez\n",
      "\t\t 144085811\n",
      "\t\t 32187701\n",
      "[18/20]: Nikolas Lessman\n",
      "\t\t 2913408\n",
      "[19/20]: Jonas Teuwen\n",
      "\t\t 32649341\n",
      "\t\t 119024451\n",
      "[20/20]: Rashindra Manniesing\n",
      "\t\t 2657081\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Find items from semantic scholar\n",
    "df_found_items = find_new_ssids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cd0dcc5-3018-4b01-b184-fc1c27686710",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df_found_items = df_found_items.drop_duplicates(subset=['ss_id'])\n",
    "# Remove items prior to 2015\n",
    "df_found_items = df_found_items[df_found_items['ss_year']>=2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dcbee23-7e27-4e70-b966-de04496eb2ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "found_items = df_found_items['ss_id'].tolist()\n",
    "found_dois = df_found_items['doi'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "265d82d3-0044-4656-9db7-0f0fea996f8f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "existing_items = return_existing_ssids(diag_bib_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdb9a950-b5d0-4a6e-be32-1c6cd9562017",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = ['bibkey', 'ss_id', 'url', 'match score', 'bib_doi', 'ss_doi', 'bib_title', 'ss_title', 'staff_id', 'staff_name', 'bib_authors', 'ss_authors', 'bib_journal', 'ss_journal', 'bib_year', 'ss_year', 'bib_type', 'ss_pmid', 'reason', 'action']\n",
    "actions_list = '[add ss_id, blacklist ss_id, add new item, add manually, None]'\n",
    "list_doi_match = []\n",
    "not_new = []\n",
    "ss_id_match = []\n",
    "for index, row in df_bib.iterrows():\n",
    "    doi = row[4]\n",
    "    ss_ids = row[8]\n",
    "    all_ss_ids = []\n",
    "    if ss_ids is not None:\n",
    "        all_ss_ids = ss_ids.split(',')\n",
    "        for i, el in enumerate(all_ss_ids):\n",
    "            all_ss_ids[i] = el.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "    \n",
    "    # Check if any existing bib-item has the same ss_id as an item on found_items -> is this correct? what if that ss_id has to be linked to another item as well?\n",
    "    for ss_id in all_ss_ids:\n",
    "        if ss_id in found_items:\n",
    "            not_new.append(ss_id)\n",
    "        \n",
    "    # Check if any existing bib-item has the same doi as an item on found_items\n",
    "    if doi is not None and doi in found_dois:\n",
    "        idx = found_dois.index(doi)\n",
    "        ss_id = found_items[idx]\n",
    "        # Check if that bib-item is already linked with the ss_id\n",
    "        if ss_id not in all_ss_ids:\n",
    "            pmid=df_found_items[df_found_items['ss_id'] ==ss_id]['pmid'].item()\n",
    "            ss_title=df_found_items[df_found_items['ss_id']==ss_id]['title'].item()\n",
    "            ss_authors = df_found_items[df_found_items['ss_id']==ss_id]['authors'].item()\n",
    "            ss_journal = df_found_items[df_found_items['ss_id']==ss_id]['journal'].item()\n",
    "            ss_year = int(df_found_items[df_found_items['ss_id']==ss_id]['ss_year'].item())\n",
    "            staff_id = int(df_found_items[df_found_items['ss_id']==ss_id]['staff_id'].item())\n",
    "            staff_name = df_found_items[df_found_items['ss_id']==ss_id]['staff_name'].item()\n",
    "            ratio = SequenceMatcher(a=ss_title,b=row[2]).ratio()\n",
    "            ss_id_match.append(ss_id)\n",
    "            list_doi_match.append((row[0], ss_id, 'https://www.semanticscholar.org/paper/'+ss_id, ratio, doi, doi, row[2], ss_title, staff_id, staff_name, row[3], ss_authors, row[6], ss_journal, row[7], ss_year, row[1], pmid, 'doi match', actions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bde765a-842f-4381-89fa-056aa366fc60",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for m in not_new:\n",
    "    title=df_found_items[df_found_items['ss_id']==m]['title'].item()\n",
    "    title_bib = ''\n",
    "    for index, row in df_bib.iterrows():\n",
    "        all_ss_ids = []\n",
    "        ss_ids = row[8]\n",
    "        if ss_ids is not None:\n",
    "            all_ss_ids = ss_ids.split(',')\n",
    "            for i, el in enumerate(all_ss_ids):\n",
    "                all_ss_ids[i] = el.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "        if m in all_ss_ids:\n",
    "            title_bib = row[2]\n",
    "            ratio = SequenceMatcher(a=title,b=title_bib).ratio()\n",
    "            if ratio < 0.9:\n",
    "                print(title_bib)\n",
    "                print(title)\n",
    "                print(ratio)\n",
    "                print('***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a3000f0-cdcb-4c23-a177-cb2fb26779fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove ss_ids that are already in bibfile and ss_id with doi match\n",
    "to_add = set(found_items)-set(not_new)-set(ss_id_match)\n",
    "new_items = df_found_items[df_found_items['ss_id'].isin(to_add)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5f8989b-4ed8-4ce4-8b4e-8597868eccdf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove blacklist items\n",
    "blacklist = pd.read_csv('script_data/blacklist.csv')\n",
    "new_items = new_items[~new_items['doi'].isin(blacklist['doi'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "497c8199-733f-4367-91f9-09b1b883e515",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dois = new_items['doi'].tolist()\n",
    "ss_ids = new_items['ss_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df748633-94d7-4692-95be-a46fe162cf49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# title match\n",
    "titles = new_items['title'].tolist()\n",
    "dois = new_items['doi'].tolist()\n",
    "ss_ids = new_items['ss_id'].tolist()\n",
    "list_title_match = []\n",
    "list_no_dois = []\n",
    "list_to_add = []\n",
    "\n",
    "for ss_id, ss_title, doi in zip(ss_ids, titles, dois):\n",
    "    title_match_ratios = df_bib['title'].apply(lambda x: SequenceMatcher(\n",
    "            a=ss_title.lower(), \n",
    "            b=x.lower().replace('{', '').replace('}', '')).ratio())\n",
    "    max_ratio = title_match_ratios.max()\n",
    "    max_bibkey = df_bib[title_match_ratios==max_ratio]['bibkey'].iloc[0]\n",
    "    max_bib_title = df_bib[title_match_ratios==max_ratio]['title'].iloc[0]\n",
    "    max_bib_title = max_bib_title.replace('{', '').replace('}', '')\n",
    "    if sum(title_match_ratios>0.8) >= 1:\n",
    "        up80_bib_entries = df_bib[title_match_ratios > 0.8]\n",
    "        for i, match in up80_bib_entries.iterrows():\n",
    "            list_title_match.append((\n",
    "                match['bibkey'],\n",
    "                ss_id,\n",
    "                f'https://www.semanticscholar.org/paper/{ss_id}',\n",
    "                title_match_ratios[i],\n",
    "                match['doi'],\n",
    "                doi,\n",
    "                match['title'].replace('{', '').replace('}', ''),\n",
    "                ss_title,\n",
    "                new_items[new_items['ss_id'] == ss_id]['staff_id'].item(),\n",
    "                new_items[new_items['ss_id'] == ss_id]['staff_name'].item(),\n",
    "                match['authors'],\n",
    "                new_items[new_items['ss_id'] == ss_id]['authors'].item(),\n",
    "                match['journal'],\n",
    "                new_items[new_items['ss_id'] == ss_id]['journal'].item(),\n",
    "                match['year'],\n",
    "                new_items[new_items['ss_id'] == ss_id]['ss_year'].item(),\n",
    "                match['type'],\n",
    "                new_items[new_items['ss_id'] == ss_id]['pmid'].item(),\n",
    "                'title match', actions_list))\n",
    "    else:\n",
    "        max_bib_entry = df_bib[title_match_ratios == max_ratio]\n",
    "        authors = max_bib_entry['authors'].iloc[0]\n",
    "        bib_doi = max_bib_entry['doi'].iloc[0]\n",
    "        max_bib_journal = max_bib_entry['journal'].iloc[0]\n",
    "        max_bib_year = max_bib_entry['year'].iloc[0]\n",
    "        type_article = max_bib_entry['type'].iloc[0]\n",
    "        \n",
    "        ss_authors = new_items[new_items['ss_id'] == ss_id]['authors'].item()\n",
    "        staff_id = new_items[new_items['ss_id'] == ss_id]['staff_id'].item()\n",
    "        staff_name = new_items[new_items['ss_id'] == ss_id]['staff_name'].item()\n",
    "        ss_journal = new_items[new_items['ss_id'] == ss_id]['journal'].item()\n",
    "        ss_year = new_items[new_items['ss_id'] == ss_id]['ss_year'].item()\n",
    "        ss_pmid = new_items[new_items['ss_id'] == ss_id]['pmid'].item()\n",
    "        \n",
    "        if doi is None:\n",
    "            list_no_dois.append((max_bibkey, ss_id, f'https://www.semanticscholar.org/paper/{ss_id}', max_ratio, bib_doi, doi, max_bib_title, ss_title, staff_id, staff_name, authors, ss_authors, max_bib_journal, ss_journal, max_bib_year, ss_year, type_article, ss_pmid, 'doi None', actions_list))\n",
    "        else:\n",
    "            list_to_add.append((max_bibkey, ss_id, f'https://www.semanticscholar.org/paper/{ss_id}', max_ratio, bib_doi, doi, max_bib_title, ss_title, staff_id, staff_name, authors, ss_authors, max_bib_journal, ss_journal, max_bib_year, ss_year, type_article, ss_pmid, 'new item', actions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0364e41f-2061-4629-bd54-d368f51a6b74",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_list = list_to_add + list_no_dois + list_title_match + list_doi_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e98fee2-1f2e-4d90-a09a-2029228624a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(total_list, columns=columns)\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "file_name = f'script_data/manual_check_{current_date}.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Remove blacklist items from manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 items removed from newly found items.\n"
     ]
    }
   ],
   "source": [
    "df = remove_blacklist_items(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1f235e4-e56c-4be6-8ec8-ee4581e6c18a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df=df.sort_values(['ss_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bd6f916-449f-4696-993a-e1d067495929",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_excel(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6b4f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
