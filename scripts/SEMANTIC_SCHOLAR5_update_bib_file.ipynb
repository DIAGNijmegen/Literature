{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from automatic_update.get_biblatex import GetBiblatex\n",
    "from bib_handling_code.processbib import read_bibfile\n",
    "from bib_handling_code.processbib import save_to_file\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "from semanticscholar import SemanticScholar, SemanticScholarException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_item_to_blacklist(item): # item here is a row from the manually checked csv file\n",
    "    #Add item to blacklist.csv\n",
    "    move_to_blacklist = {\n",
    "        'staff_id': item.get('staff_id', None),\n",
    "        'staff_name': item.get('staff_id', None),\n",
    "        'ss_year': item.get('ss_year', None),\n",
    "        'ss_id': item.get('ss_id', None),\n",
    "        'title': item.get('ss_title', None),\n",
    "        'doi': item.get('ss_doi', None),\n",
    "        'Should be in diag.bib': 'no',\n",
    "        'Reason': item.get('Blacklist reason', None)\n",
    "    }\n",
    "\n",
    "    return move_to_blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_blacklist_csv(blacklist_df, blacklist_entries, blacklist_out_file): #blacklist_csv is a df\n",
    "    # Add all items to blacklist.csv\n",
    "    blacklist_df = pd.concat([blacklist_df, pd.DataFrame(blacklist_entries)], ignore_index=True)\n",
    "\n",
    "    # Save blacklist.csv\n",
    "    blacklist_df.to_csv(blacklist_out_file, index=False)\n",
    "    return f\"{len(blacklist_entries)} items added to blacklist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code to get citations from semantic scholar. If there are multiple ss_ids, we should get the number of citations for each of them and sum the two (or more?) values.\n",
    "def get_citations(semantic_scholar_ids, sch):\n",
    "    dict_cits = {}\n",
    "    ss_ids_not_found = []\n",
    "    for ss_id in semantic_scholar_ids:\n",
    "        tries = 8\n",
    "        i=0\n",
    "        while i<tries:\n",
    "            print('trying time', i, ss_id)\n",
    "            try:\n",
    "                paper = sch.get_paper(ss_id)\n",
    "                paper_id = paper['paperId']\n",
    "                dict_cits[paper_id] = len(paper['citations'])\n",
    "                print('success getting citations')\n",
    "                i=tries # we succeeded so max out the tries\n",
    "            except SemanticScholarException.ObjectNotFoundException as onfe:\n",
    "                ss_ids_not_found.append(ss_id)\n",
    "                print('failed cleanly to get citations')\n",
    "                i=tries # we failed cleanly so max out the tries\n",
    "            except Exception as e: # some kind of time out error\n",
    "                print('failed to get citations, trying again')\n",
    "                i = i+1  # if we still have more tries left then try it again\n",
    "            \n",
    "    return dict_cits, ss_ids_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_bib_info(diag_bib_file, item): #diag_bib_file is the file read in as a string, item is row from csv\n",
    "    #Get DOI information\n",
    "\n",
    "    # if no ss_doi exists\n",
    "    if len(str(item['ss_doi']))==0 or str(item['ss_doi'])=='nan':\n",
    "        print('no ss_doi available, I cannot add new bib entry', item['ss_id'])\n",
    "        return None\n",
    "    \n",
    "    # make sure doi is not already in diag.bib\n",
    "    if item['ss_doi'] in diag_bib_file:\n",
    "\n",
    "        start_index = diag_bib_file.find(item['ss_doi'])\n",
    "        end_index = diag_bib_file.find('}', start_index)  # Include the closing brace\n",
    "        matching_item_str = diag_bib_file[start_index:end_index]\n",
    "\n",
    "        print('DOI already exists in bib file. Matching item:', matching_item_str)\n",
    "\n",
    "        if matching_item_str == item['ss_doi']:\n",
    "            print('doi already exists in bib file, I will not add new bib entry', item['ss_doi'], item['ss_id'])\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            print('similar doi already exists in bib file, but new item will be added for ', item['ss_doi'], item['ss_id'])\n",
    "\n",
    "    # Get BibLatex information based on DOI if not in the file\n",
    "    reader = GetBiblatex(doi=item['ss_doi'], diag_bib=diag_bib_file)\n",
    "    bibtext = reader.get_bib_text()\n",
    "\n",
    "    # Return the bibtext if it is not 'empty', otherwise return None\n",
    "    return bibtext if bibtext != 'empty' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_ss_id_doi_pmid_to_existing_bibkey(diag_bib_raw, item_row):\n",
    "    ss_id = item_row['ss_id']\n",
    "    bibkey = item_row['bibkey']\n",
    "    #Update bibkey with ss_id\n",
    "    for ind, entry in enumerate(diag_bib_raw):\n",
    "        if entry.type == 'string':\n",
    "            continue\n",
    "\n",
    "        # if we found the relevant key\n",
    "        if bibkey == entry.key:\n",
    "            # print('entry matched is ', entry.fields)\n",
    "            # if there is already something in all_ss_ids\n",
    "            if 'all_ss_ids' in entry.fields.keys():\n",
    "                if not entry.fields['all_ss_ids'] == '{' + str(ss_id) + '}': # this should never happen, right? (from Keelin!)\n",
    "                    previous = literal_eval(entry.fields['all_ss_ids'].strip('{}'))\n",
    "                    new = ss_id\n",
    "                    combined = list(set(previous) | set([new]))\n",
    "                    # update the entry\n",
    "                    entry.fields['all_ss_ids'] = '{' + str(combined) + '}'\n",
    "            # if there is no ss_id here yet just add this single one\n",
    "            else:   \n",
    "                    entry.fields['all_ss_ids'] = '{[' + str(ss_id) + ']}'\n",
    "            print(str(ss_id), 'added to diag_bib_raw')\n",
    "\n",
    "            ss_doi = str(item_row['ss_doi']).strip()\n",
    "            if not 'doi' in entry.fields.keys() and len(ss_doi)>0:\n",
    "                print('will add doi to bibkey', bibkey,  ss_doi)\n",
    "                entry.fields['doi'] = '{' + ss_doi + '}'\n",
    "            ss_pmid = item_row['ss_pmid'].strip()\n",
    "            if not 'pmid' in entry.fields.keys() and len(ss_pmid)>0:\n",
    "                print('will add pmid to bibkey', bibkey,  ss_pmid)\n",
    "                entry.fields['pmid'] = '{' + ss_pmid + '}'\n",
    "\n",
    "\n",
    "            return [diag_bib_raw, 'Success']\n",
    "        \n",
    "    # if we haven't returned by now then we failed to update \n",
    "    print('failed to add ss_id to diag.bib', str(ss_id), str(bibkey))\n",
    "    return [diag_bib_raw, 'Fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ss_id_and_pmid_where_possible(diag_bib_raw, dict_new_items_bibkey_ss_id_and_pmid):\n",
    "    # iterate through all items in the diag bib and update them if we have missing information on them\n",
    "    for ind, entry in enumerate(diag_bib_raw):\n",
    "        if entry.type == 'string':\n",
    "            continue\n",
    "\n",
    "        # if we found the relevant key\n",
    "        current_bibkey = entry.key\n",
    "        if current_bibkey in dict_new_items_bibkey_ss_id_and_pmid.keys():\n",
    "            ss_id = dict_new_items_bibkey_ss_id_and_pmid[current_bibkey]['ss_id'].strip()\n",
    "            print('will add ss_id to bibkey', current_bibkey, ss_id)\n",
    "            if 'all_ss_ids' in entry.fields.keys():\n",
    "                if not entry.fields['all_ss_ids'] == '{' + str(ss_id) + '}': # this should never happen, right? (from Keelin!)\n",
    "                    previous = literal_eval(entry.fields['all_ss_ids'].strip('{}'))\n",
    "                    new = ss_id\n",
    "                    combined = list(set(previous) | set([new]))\n",
    "                    # update the entry\n",
    "                    entry.fields['all_ss_ids'] = '{' + str(combined) + '}'\n",
    "            # if there is no ss_id here yet just add this single one\n",
    "            else:   \n",
    "                    entry.fields['all_ss_ids'] = '{[' + str(ss_id) + ']}'\n",
    "                    \n",
    "            if not 'pmid' in entry.fields.keys() and 'ss_pmid' in dict_new_items_bibkey_ss_id_and_pmid[current_bibkey].keys():\n",
    "                print('will add pmid to bibkey', dict_new_items_bibkey_ss_id_and_pmid[current_bibkey]['ss_pmid'])\n",
    "                entry.fields['pmid'] = '{' + dict_new_items_bibkey_ss_id_and_pmid[current_bibkey]['ss_pmid'] + '}'\n",
    "\n",
    "    return diag_bib_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_citation_count(diag_bib_raw):\n",
    "    all_ss_ids_not_found = []\n",
    "    num_entries = len(diag_bib_raw)\n",
    "\n",
    "    sch = SemanticScholar(timeout=40)\n",
    "    sch.timeout=40\n",
    "\n",
    "    for ind, entry in enumerate(diag_bib_raw):\n",
    "        # print('checking citations', ind, 'of', num_entries)\n",
    "        flag=0\n",
    "        if entry.type == 'string':\n",
    "            continue\n",
    "        if 'all_ss_ids' in entry.fields:\n",
    "            all_ss_ids = []\n",
    "            ss_ids = entry.fields['all_ss_ids'].translate(str.maketrans('', '', string.punctuation)).split(' ')\n",
    "            if len(ss_ids) > 1:\n",
    "                all_ss_ids.extend(ss_ids)\n",
    "            else:\n",
    "                all_ss_ids.append(ss_ids[0])\n",
    "            print('trying with key', entry.key, 'and ss ids', all_ss_ids)\n",
    "            dict_cits, ss_ids_not_found_this_item = get_citations(all_ss_ids, sch)\n",
    "            if len(ss_ids_not_found_this_item)>0:\n",
    "                print('adding items to ss_ids_not_found', ss_ids_not_found_this_item)\n",
    "                all_ss_ids_not_found.extend(ss_ids_not_found_this_item)\n",
    "            n_cits = 0\n",
    "            for key in dict_cits.keys():\n",
    "                n_cits += dict_cits[key]\n",
    "            print('n_cits this item is ', n_cits)\n",
    "            # TODO: is it correct logic to use this field name or should we make a new one?\n",
    "            if 'gscites' in entry.fields:\n",
    "                # only update if we are increasing the number of citations!!!\n",
    "                previous_cits = int(entry.fields['gscites'].strip('{}'))\n",
    "                if n_cits > previous_cits:\n",
    "                    print('updating', entry.key, 'from', previous_cits, 'to', n_cits)\n",
    "                    entry.fields['gscites'] = '{' + str(n_cits) + '}'\n",
    "                elif (previous_cits > (1.5 * n_cits)) and (previous_cits - n_cits > 10):\n",
    "                    print('warning: num citations calculated for this bibkey is much lower than previously suggested....', entry.key, previous_cits, n_cits)\n",
    "                else:\n",
    "                    print('will not update', entry.key, 'as there is no increase', n_cits, previous_cits)\n",
    "            else:\n",
    "                print('adding gscites', entry.key, n_cits)\n",
    "                entry.fields['gscites'] = '{' + str(n_cits) + '}'\n",
    "    print('done updating citations')\n",
    "    return diag_bib_raw, all_ss_ids_not_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load manually checked csv file and bib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load manually_checked\n",
    "manually_checked = pd.read_excel(\"./script_data/manual_check_20231018.xlsx\")\n",
    "manually_checked['ss_pmid'] = manually_checked['ss_pmid'].fillna('-1')\n",
    "manually_checked['ss_pmid'] = manually_checked['ss_pmid'].astype(int).astype(str)\n",
    "manually_checked['ss_pmid'] = manually_checked['ss_pmid'].replace('-1', '')\n",
    "\n",
    "manually_checked['ss_doi'] = manually_checked['ss_doi'].fillna('')\n",
    "\n",
    "\n",
    "# load bib file just for reading at this point\n",
    "#TODO: in the end when this script is routine this should just read the live diag.bib\n",
    "cwd = os.getcwd()\n",
    "parent_directory = os.path.dirname(cwd)\n",
    "diag_bib_path = os.path.join(parent_directory, 'scripts/script_data/diag_ss.bib')\n",
    "with open(diag_bib_path, 'r', encoding=\"utf8\") as orig_bib_file:\n",
    "    diag_bib_str = orig_bib_file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through all items in the manually checked csv\n",
    "blacklist_items = []\n",
    "num_items_added = 0\n",
    "items_to_update = []\n",
    "\n",
    "failed_new_items = []\n",
    "failed_updated_items = []\n",
    "failed_to_find_actions = []\n",
    "\n",
    "dict_new_items_bibkey_ss_id_and_pmid = {}\n",
    "\n",
    "\n",
    "for index, bib_item in manually_checked.iterrows():\n",
    "    print(f\"Working on {index}/{len(manually_checked)}: {bib_item['ss_doi']} (action is {bib_item['action']})\")\n",
    "    # Make sure item is manually checked\n",
    "    if \",\" in bib_item['action']:\n",
    "        print(f\"{bib_item['ss_id']} has not been checked yet, make sure only 1 action is mentioned\")\n",
    "        failed_to_find_actions.append(bib_item)\n",
    "        continue\n",
    "\n",
    "    # Add new item to diag.bib\n",
    "    elif \"[add new item]\" == bib_item['action'].strip():\n",
    "       \n",
    "       bib_item_text = get_bib_info(diag_bib_str, bib_item)\n",
    "\n",
    "       # @Dre it seems like failure results in the return of the text 'empty' - could we return None instead? \n",
    "       if bib_item_text is not None:\n",
    "           num_items_added += 1\n",
    "           # update the diag_bib_str immediately so that the bibkey cannot be reused for future additions in this loop\n",
    "           diag_bib_str = diag_bib_str + bib_item_text\n",
    "           # if there is a pmid note it to be added afterwards\n",
    "           ss_pmid = str(bib_item['ss_pmid']).strip()\n",
    "           ss_id = str(bib_item['ss_id']).strip() # there must always be an ss_id\n",
    "           # bit of a hacky way to get the bibkey of the added item\n",
    "           bibkey_added = bib_item_text[bib_item_text.index('{')+1:bib_item_text.index(',')]\n",
    "           dict_new_items_bibkey_ss_id_and_pmid[bibkey_added]={'ss_id':ss_id}\n",
    "           if len(ss_pmid)>0:\n",
    "               dict_new_items_bibkey_ss_id_and_pmid[bibkey_added]['ss_pmid'] = ss_pmid\n",
    "           print('storing bibkey, ss_id and pmid', bibkey_added, ss_id, ss_pmid)\n",
    "       else:\n",
    "           print('failed to find details for doi, ss_id', bib_item['ss_doi'], bib_item['ss_id'])\n",
    "           failed_new_items.append(bib_item)\n",
    "       \n",
    "\n",
    "    # Add ss_id to already existing doi in diag.bib\n",
    "    elif \"[add ss_id]\" in bib_item['action'].strip():\n",
    "        # just store a list of these items for now and we will update the file at the end\n",
    "        items_to_update += [bib_item]\n",
    "        \n",
    "    # Get items to blacklist\n",
    "    elif \"blacklist\" in bib_item['action'].strip():\n",
    "        blacklist_item = get_item_to_blacklist(bib_item)\n",
    "        blacklist_items.append(blacklist_item)\n",
    "\n",
    "    # Get None items\n",
    "    elif '[None]' in bib_item['action'].strip():\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print('failed to find action', bib_item['action'])\n",
    "        failed_to_find_actions.append(bib_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Save the file with new bib items in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save the file to disk \n",
    "# TODO : write to correct location\n",
    "diag_bib_path_tmp_new = os.path.join(parent_directory, 'scripts/script_data/diag_ss_tmp_new.bib')\n",
    "with open(diag_bib_path_tmp_new, 'w', encoding=\"utf8\") as bibtex_file:\n",
    "    bibtex_file.write(diag_bib_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update newly added items with ss_id and pmids where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we re-open the bib file using the read_bibfile method and update newly added items with new pmids\n",
    "# TODO read from correct location here\n",
    "diag_bib_raw = read_bibfile(None, diag_bib_path_tmp_new)\n",
    "diag_bib_raw = add_ss_id_and_pmid_where_possible(diag_bib_raw, dict_new_items_bibkey_ss_id_and_pmid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update existing bib entries with new ss_ids (and dois, pmids where possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Next we go over existing items that need ss_ids added (and possibly doi, pmid at same time)\n",
    "for item_to_update in items_to_update:\n",
    "    [diag_bib_raw, result] = add_ss_id_doi_pmid_to_existing_bibkey(diag_bib_raw, item_to_update)\n",
    "    if(result=='Fail'):\n",
    "        failed_updated_items.append(item_to_update)\n",
    "\n",
    "\n",
    "#Note we can remove this write - it is just for safety /debug right now\n",
    "save_to_file(diag_bib_raw, None, diag_bib_path_tmp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update citation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_ids_not_found_for_citations =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "diag_bib_raw_new_cits, ss_ids_not_found_for_citations = update_citation_count(diag_bib_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: update to the correct output path\n",
    "save_to_file(diag_bib_raw_new_cits, None, diag_bib_path_tmp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update the blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Last we update the blacklist (temporarily commented) (what failures can happen here?)\n",
    "blacklist_df = pd.read_csv('./script_data/blacklist.csv')\n",
    "# TODO: fix to correct output location\n",
    "blacklist_out_file = './script_data/blacklist_tmp_updated.csv'\n",
    "# file writing\n",
    "update_blacklist_csv(blacklist_df, blacklist_items, blacklist_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Here we provide a report of rows where we did not know what to do or we failed to do the action\n",
    "print(\"DONE with processing manually checked items\")\n",
    "print('Failures are as follows:')\n",
    "for item in failed_new_items:\n",
    "    print('Failed to add new bib entry ', item['ss_id'])\n",
    "for item in failed_updated_items:\n",
    "    print('Failed to update exiting bib entry with new ss_id', item['bibkey'], item['ss_id'])\n",
    "for item in failed_to_find_actions:\n",
    "    print('Failed to find valid action for item', item['ss_id'], item['action'])\n",
    "for item in ss_ids_not_found_for_citations:\n",
    "    print('Failed to find this ss_id to update citations', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Blacklisted items: {len(blacklist_items)}\")\n",
    "print(f\"Updated items: {len(items_to_update)}\")\n",
    "print(f\"Newly added items: {num_items_added}\")\n",
    "import numpy as np\n",
    "count_action_none = np.sum(np.fromiter(('none' in str(action).lower() for action in manually_checked['action']), dtype=bool))\n",
    "print(f\"Items with action None: {count_action_none}\")\n",
    "\n",
    "\n",
    "print(f\"total processed items: {len(blacklist_items) + len(items_to_update) + num_items_added + len(failed_new_items) + len(failed_updated_items) + len(failed_to_find_actions) + count_action_none}\")\n",
    "print(f\"amount of items in manual checkfile: {manually_checked.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: update to the correct output path\n",
    "save_to_file(diag_bib_raw_new_cits, None, diag_bib_path_tmp_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
