{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from get_biblatex import GetBiblatex\n",
    "from bib_handling_code.processbib import read_bibfile\n",
    "from bib_handling_code.processbib import save_to_file\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "from semanticscholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' KM: Remove this fn from here. Dre to update GenerateCSVFile with method like this which handles no-doi items\\ndef remove_blacklist_items(df_new_items):\\n    blacklisted_items = pd.read_csv(\"./script_data/blacklist.csv\")\\n    initial_length = len(df_new_items)\\n    df_new_items = df_new_items[~df_new_items[\\'ss_doi\\'].isin(blacklisted_items[\\'doi\\'].unique().tolist())] # remove blacklisted dois\\n    df_new_items = df_new_items[~df_new_items[\\'ss_id\\'].isin(blacklisted_items[\\'ss_id\\'].unique().tolist())] # remove blacklisted dois\\n\\n    print(f\"{initial_length-len(df_new_items)} items removed from newly found items.\")\\n    return df_new_items\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' KM: Remove this fn from here. Dre to update GenerateCSVFile with method like this which handles no-doi items\n",
    "def remove_blacklist_items(df_new_items):\n",
    "    blacklisted_items = pd.read_csv(\"./script_data/blacklist.csv\")\n",
    "    initial_length = len(df_new_items)\n",
    "    df_new_items = df_new_items[~df_new_items['ss_doi'].isin(blacklisted_items['doi'].unique().tolist())] # remove blacklisted dois\n",
    "    df_new_items = df_new_items[~df_new_items['ss_id'].isin(blacklisted_items['ss_id'].unique().tolist())] # remove blacklisted dois\n",
    "\n",
    "    print(f\"{initial_length-len(df_new_items)} items removed from newly found items.\")\n",
    "    return df_new_items\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_item_to_blacklist(item): # item here is a row from the manually checked csv file\n",
    "    #Add item to blacklist.csv\n",
    "    move_to_blacklist = {\n",
    "        'staff_id': item.get('staff_id', None),\n",
    "        'staff_name': item.get('staff_id', None),\n",
    "        'ss_year': item.get('ss_year', None),\n",
    "        'ss_id': item.get('ss_id', None),\n",
    "        'title': item.get('ss_title', None),\n",
    "        'doi': item.get('ss_doi', None),\n",
    "        'Should be in diag.bib': 'no',\n",
    "        'Reason': item.get('Blacklist reason', None)\n",
    "    }\n",
    "\n",
    "    return move_to_blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_blacklist_csv(blacklist_df, blacklist_entries, blacklist_out_file): #blacklist_csv is a df\n",
    "    # Add all items to blacklist.csv\n",
    "    blacklist_df = pd.concat([blacklist_df, pd.DataFrame(blacklist_entries)], ignore_index=True)\n",
    "\n",
    "    # Save blacklist.csv\n",
    "    blacklist_df.to_csv(blacklist_out_file, index=False)\n",
    "    return f\"{len(blacklist_entries)} items added to blacklist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code to get citations from semantic scholar. If there are multiple ss_ids, we should get the number of citations for each of them and sum the two (or more?) values.\n",
    "def get_citations(semantic_scholar_ids):\n",
    "    dict_cits = {}\n",
    "    for ss_id in semantic_scholar_ids:\n",
    "        sch = SemanticScholar()\n",
    "        paper = sch.get_paper(ss_id)\n",
    "        paper_id = paper['paperId']\n",
    "        dict_cits[paper_id] = len(paper['citations'])\n",
    "    return dict_cits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_bib_info(diag_bib_file, item): #diag_bib_file is the file read in as a string, item is row from csv\n",
    "    #Get DOI information\n",
    "\n",
    "    # if no ss_doi exists\n",
    "    if len(str(item['ss_doi']))==0 or str(item['ss_doi'])=='nan':\n",
    "        print('no ss_doi available, I cannot add new bib entry', item['ss_id'])\n",
    "        return None\n",
    "    \n",
    "    # make sure doi is not already in diag.bib\n",
    "    if item['ss_doi'] in diag_bib_file:\n",
    "\n",
    "        start_index = diag_bib_file.find(item['ss_doi'])\n",
    "        end_index = diag_bib_file.find('}', start_index)  # Include the closing brace\n",
    "        matching_item_str = diag_bib_file[start_index:end_index]\n",
    "\n",
    "        print('DOI already exists in bib file. Matching item:', matching_item_str)\n",
    "\n",
    "        if matching_item_str == item['ss_doi']:\n",
    "            print('doi already exists in bib file, I will not add new bib entry', item['ss_doi'], item['ss_id'])\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            print('similar doi already exists in bib file, but new item will be added for ', item['ss_doi'], item['ss_id'])\n",
    "\n",
    "    # Get BibLatex information based on DOI if not in the file\n",
    "    reader = GetBiblatex(doi=item['ss_doi'], diag_bib=diag_bib_file)\n",
    "    bibtext = reader.get_bib_text()\n",
    "\n",
    "    # Return the bibtext if it is not 'empty', otherwise return None\n",
    "    return bibtext if bibtext != 'empty' else None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_ss_id_to_existing_bibkey(diag_bib_raw, ss_id, bibkey):\n",
    "    \n",
    "    #Update bibkey with ss_id\n",
    "    for ind, entry in enumerate(diag_bib_raw):\n",
    "        if entry.type == 'string':\n",
    "            continue\n",
    "\n",
    "        # if we found the relevant key\n",
    "        if bibkey == entry.key:\n",
    "            # if there is already something in all_ss_ids\n",
    "            if 'all_ss_ids' in entry.fields.keys():\n",
    "                if not entry.fields['all_ss_ids'] == '{' + str(ss_id) + '}': # this should never happen, right? (from Keelin!)\n",
    "                    previous = literal_eval(entry.fields['all_ss_ids'].strip('{}'))\n",
    "                    new = ss_id\n",
    "                    combined = list(set(previous) | set([new]))\n",
    "                    # update the entry\n",
    "                    entry.fields['all_ss_ids'] = '{' + str(combined) + '}'\n",
    "            # if there is no ss_id here yet just add this single one\n",
    "            else:   \n",
    "                    entry.fields['all_ss_ids'] = '{' + str(ss_id) + '}'\n",
    "            print(str(ss_id), 'added to diag_bib_raw')\n",
    "            return [diag_bib_raw, 'Success']\n",
    "        \n",
    "    # if we haven't returned by now then we failed to update \n",
    "    print('failed to add ss_id to diag.bib', str(ss_id), str(bibkey))\n",
    "    return [diag_bib_raw, 'Fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_citation_count(diag_bib_raw):\n",
    "    num_entries = len(diag_bib_raw)\n",
    "    for ind, entry in enumerate(diag_bib_raw):\n",
    "        # print('checking citations', ind, 'of', num_entries)\n",
    "        flag=0\n",
    "        if entry.type == 'string':\n",
    "            continue\n",
    "        if 'all_ss_ids' in entry.fields:\n",
    "            all_ss_ids = []\n",
    "            ss_ids = entry.fields['all_ss_ids'].translate(str.maketrans('', '', string.punctuation)).split(' ')\n",
    "            if len(ss_ids) > 1:\n",
    "                all_ss_ids.extend(ss_ids)\n",
    "            else:\n",
    "                all_ss_ids.append(ss_ids[0])\n",
    "            dict_cits = get_citations(all_ss_ids)\n",
    "            n_cits = 0\n",
    "            for key in dict_cits.keys():\n",
    "                n_cits += dict_cits[key]\n",
    "            # TODO: is it correct logic to use this field name or should we make a new one?\n",
    "            if 'gscites' in entry.fields:\n",
    "                # only update if we are increasing the number of citations!!!\n",
    "                previous_cits = int(entry.fields['gscites'].strip('{}'))\n",
    "                if n_cits > previous_cits:\n",
    "                    print('updating', entry.key, 'from', previous_cits, 'to', n_cits)\n",
    "                    entry.fields['gscites'] = '{' + str(n_cits) + '}'\n",
    "                elif (previous_cits > (1.5 * n_cits)) and (previous_cits - n_cits > 10):\n",
    "                    print('warning: num citations calculated for this bibkey is much lower than previously suggested....', entry.key, previous_cits, n_cits)\n",
    "            else:\n",
    "                print('adding gscites', entry.key, n_cits)\n",
    "                entry.fields['gscites'] = '{' + str(n_cits) + '}'\n",
    "    print('done updating citations')\n",
    "    return diag_bib_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load manually checked csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load manually_checked\n",
    "manually_checked = pd.read_excel(\"./script_data/manual_check_20231018.xlsx\")\n",
    "# manually_checked = remove_blacklist_items(manually_checked)     # This should be done before actually manually checking\n",
    "\n",
    "# load bib file just for reading at this point\n",
    "#TODO: in the end when this script is routine this should just read the live diag.bib\n",
    "cwd = os.getcwd()\n",
    "parent_directory = os.path.dirname(cwd)\n",
    "diag_bib_path = os.path.join(parent_directory, 'scripts/script_data/diag_ss.bib')\n",
    "with open(diag_bib_path, 'r', encoding=\"utf8\") as readonly_bib_file:\n",
    "    diag_bib_readonly = readonly_bib_file.read()\n",
    "    \n",
    "# POTENTIAL TO-DO CREATE ACTION MAPPINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0/280\n",
      "Working on 1/280\n",
      "Working on 2/280\n",
      "Working on 3/280\n",
      "Working on 4/280\n",
      "Working on 5/280\n",
      "Working on 6/280\n",
      "Working on 7/280\n",
      "Working on 8/280\n",
      "Working on 9/280\n",
      "Working on 10/280\n",
      "Working on 11/280\n",
      "Working on 12/280\n",
      "20\n",
      "Working on 13/280\n",
      "Working on 14/280\n",
      "308\n",
      "Working on 15/280\n",
      "Working on 16/280\n",
      "Working on 17/280\n",
      "Working on 18/280\n",
      "Working on 19/280\n",
      "Working on 20/280\n",
      "Working on 21/280\n",
      "Working on 22/280\n",
      "Working on 23/280\n",
      "Working on 24/280\n",
      "Working on 25/280\n",
      "Working on 26/280\n",
      "Working on 27/280\n",
      "Working on 28/280\n",
      "Working on 29/280\n",
      "Working on 30/280\n",
      "Working on 31/280\n",
      "DOI already exists in bib file. Matching item: 10.1109/TMI.2016.2553401\n",
      "doi already exists in bib file, I will not add new bib entry 10.1109/TMI.2016.2553401 1d2109f8ec43c23db647c4778a5bb5846074e575\n",
      "Working on 32/280\n",
      "Working on 33/280\n",
      "Working on 34/280\n",
      "Working on 35/280\n",
      "Working on 36/280\n",
      "Working on 37/280\n",
      "Working on 38/280\n",
      "Working on 39/280\n",
      "Working on 40/280\n",
      "Working on 41/280\n",
      "Working on 42/280\n",
      "Working on 43/280\n",
      "Working on 44/280\n",
      "Working on 45/280\n",
      "Working on 46/280\n",
      "Working on 47/280\n",
      "Working on 48/280\n",
      "Working on 49/280\n",
      "Working on 50/280\n",
      "Working on 51/280\n",
      "Working on 52/280\n",
      "Working on 53/280\n",
      "Working on 54/280\n",
      "Working on 55/280\n",
      "Working on 56/280\n",
      "Working on 57/280\n",
      "Working on 58/280\n",
      "Working on 59/280\n",
      "Working on 60/280\n",
      "Working on 61/280\n",
      "16\n",
      "Working on 62/280\n",
      "Working on 63/280\n",
      "Working on 64/280\n",
      "8\n",
      "Working on 65/280\n",
      "Working on 66/280\n",
      "Working on 67/280\n",
      "Working on 68/280\n",
      "Working on 69/280\n",
      "15\n",
      "Working on 70/280\n",
      "21\n",
      "Working on 71/280\n",
      "Working on 72/280\n",
      "Working on 73/280\n",
      "Working on 74/280\n",
      "Working on 75/280\n",
      "Working on 76/280\n",
      "Unable to generate bibtext for 10.1201/B18191-7\n",
      "'author'\n",
      "Working on 77/280\n",
      "Working on 78/280\n",
      "Working on 79/280\n",
      "Working on 80/280\n",
      "Working on 81/280\n",
      "Working on 82/280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 83/280\n",
      "8\n",
      "Working on 84/280\n",
      "Working on 85/280\n",
      "Working on 86/280\n",
      "Working on 87/280\n",
      "Working on 88/280\n",
      "Working on 89/280\n",
      "Working on 90/280\n",
      "Working on 91/280\n",
      "Working on 92/280\n",
      "Working on 93/280\n",
      "Working on 94/280\n",
      "Working on 95/280\n",
      "Working on 96/280\n",
      "Working on 97/280\n",
      "Working on 98/280\n",
      "Working on 99/280\n",
      "Working on 100/280\n",
      "Working on 101/280\n",
      "Working on 102/280\n",
      "Working on 103/280\n",
      "Working on 104/280\n",
      "Working on 105/280\n",
      "16\n",
      "Working on 106/280\n",
      "Working on 107/280\n",
      "Working on 108/280\n",
      "failed to find action\n",
      "Working on 109/280\n",
      "Working on 110/280\n",
      "failed to find action\n",
      "Working on 111/280\n",
      "Working on 112/280\n",
      "Working on 113/280\n",
      "Working on 114/280\n",
      "Working on 115/280\n",
      "Working on 116/280\n",
      "17\n",
      "Working on 117/280\n",
      "Working on 118/280\n",
      "Working on 119/280\n",
      "Working on 120/280\n",
      "Working on 121/280\n",
      "Working on 122/280\n",
      "Working on 123/280\n",
      "Working on 124/280\n",
      "Working on 125/280\n",
      "Working on 126/280\n",
      "Working on 127/280\n",
      "Working on 128/280\n",
      "Working on 129/280\n",
      "Working on 130/280\n",
      "Working on 131/280\n",
      "Working on 132/280\n",
      "Working on 133/280\n",
      "8\n",
      "Working on 134/280\n",
      "failed to find action\n",
      "Working on 135/280\n",
      "Working on 136/280\n",
      "Working on 137/280\n",
      "Working on 138/280\n",
      "Working on 139/280\n",
      "Working on 140/280\n",
      "Working on 141/280\n",
      "Working on 142/280\n",
      "Working on 143/280\n",
      "Working on 144/280\n",
      "Working on 145/280\n",
      "Working on 146/280\n",
      "Working on 147/280\n",
      "Working on 148/280\n",
      "Working on 149/280\n",
      "Working on 150/280\n",
      "Working on 151/280\n",
      "failed to find action\n",
      "Working on 152/280\n",
      "Working on 153/280\n",
      "failed to find action\n",
      "Working on 154/280\n",
      "Working on 155/280\n",
      "9\n",
      "Working on 156/280\n",
      "Working on 157/280\n",
      "Working on 158/280\n",
      "Working on 159/280\n",
      "Working on 160/280\n",
      "Working on 161/280\n",
      "1\n",
      "Working on 162/280\n",
      "Working on 163/280\n",
      "Working on 164/280\n",
      "Working on 165/280\n",
      "Working on 166/280\n",
      "Working on 167/280\n",
      "Working on 168/280\n",
      "Working on 169/280\n",
      "Working on 170/280\n",
      "Working on 171/280\n",
      "Working on 172/280\n",
      "failed to find action\n",
      "Working on 173/280\n",
      "Working on 174/280\n",
      "Working on 175/280\n",
      "failed to find action\n",
      "Working on 176/280\n",
      "Working on 177/280\n",
      "Working on 178/280\n",
      "Working on 179/280\n",
      "failed to find action\n",
      "Working on 180/280\n",
      "Working on 181/280\n",
      "Working on 182/280\n",
      "Working on 183/280\n",
      "Working on 184/280\n",
      "Working on 185/280\n",
      "Working on 186/280\n",
      "Working on 187/280\n",
      "Working on 188/280\n",
      "Working on 189/280\n",
      "Working on 190/280\n",
      "Working on 191/280\n",
      "Working on 192/280\n",
      "Working on 193/280\n",
      "Working on 194/280\n",
      "Working on 195/280\n",
      "Working on 196/280\n",
      "Working on 197/280\n",
      "Working on 198/280\n",
      "Working on 199/280\n",
      "Working on 200/280\n",
      "Working on 201/280\n",
      "Working on 202/280\n",
      "Working on 203/280\n",
      "Working on 204/280\n",
      "Working on 205/280\n",
      "Working on 206/280\n",
      "Working on 207/280\n",
      "Working on 208/280\n",
      "failed to find action\n",
      "Working on 209/280\n",
      "Working on 210/280\n",
      "Working on 211/280\n",
      "Working on 212/280\n",
      "Working on 213/280\n",
      "Working on 214/280\n",
      "Unable to generate bibtext for 10.1101/2022.05.17.492245\n",
      "local variable 'kind' referenced before assignment\n",
      "Working on 215/280\n",
      "Working on 216/280\n",
      "Working on 217/280\n",
      "Working on 218/280\n",
      "Working on 219/280\n",
      "Working on 220/280\n",
      "21\n",
      "Working on 221/280\n",
      "Working on 222/280\n",
      "Working on 223/280\n",
      "Working on 224/280\n",
      "Working on 225/280\n",
      "Working on 226/280\n",
      "Working on 227/280\n",
      "Working on 228/280\n",
      "Working on 229/280\n",
      "failed to find action\n",
      "Working on 230/280\n",
      "Working on 231/280\n",
      "Working on 232/280\n",
      "Working on 233/280\n",
      "Working on 234/280\n",
      "Working on 235/280\n",
      "Working on 236/280\n",
      "Working on 237/280\n",
      "Working on 238/280\n",
      "Working on 239/280\n",
      "Working on 240/280\n",
      "Working on 241/280\n",
      "Working on 242/280\n",
      "failed to find action\n",
      "Working on 243/280\n",
      "Working on 244/280\n",
      "Working on 245/280\n",
      "Working on 246/280\n",
      "Working on 247/280\n",
      "Working on 248/280\n",
      "Working on 249/280\n",
      "Working on 250/280\n",
      "failed to find action\n",
      "Working on 251/280\n",
      "Working on 252/280\n",
      "Working on 253/280\n",
      "Working on 254/280\n",
      "DOI already exists in bib file. Matching item: 10.1007/978-3-030-00949-6_16\n",
      "similar doi already exists in bib file, but new item will be added for  10.1007/978-3-030-00949-6 e717ffb38990d5da64a82f8b8715bfb56daf9762\n",
      "Unable to generate bibtext for 10.1007/978-3-030-00949-6\n",
      "'author'\n",
      "Working on 255/280\n",
      "Working on 256/280\n",
      "Working on 257/280\n",
      "Unable to generate bibtext for 10.1101/2022.09.02.22279476\n",
      "local variable 'kind' referenced before assignment\n",
      "Working on 258/280\n",
      "Unable to generate bibtext for 10.1055/B000000232\n",
      "'author'\n",
      "Working on 259/280\n",
      "Working on 260/280\n",
      "Working on 261/280\n",
      "Working on 262/280\n",
      "Working on 263/280\n",
      "Working on 264/280\n",
      "3\n",
      "Working on 265/280\n",
      "Working on 266/280\n",
      "Working on 267/280\n",
      "Working on 268/280\n",
      "Working on 269/280\n",
      "Working on 270/280\n",
      "Working on 271/280\n",
      "Working on 272/280\n",
      "Unable to generate bibtext for 10.1101/158014\n",
      "local variable 'kind' referenced before assignment\n",
      "Working on 273/280\n",
      "Working on 274/280\n",
      "Working on 275/280\n",
      "Working on 276/280\n",
      "Working on 277/280\n",
      "24\n",
      "Working on 278/280\n",
      "Working on 279/280\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all items in the manually checked csv\n",
    "blacklist_items = []\n",
    "items_to_add = ''\n",
    "items_to_update = []\n",
    "\n",
    "failed_new_items = []\n",
    "failed_updated_items = []\n",
    "failed_to_find_actions = []\n",
    "\n",
    "#TODO: Make sure new items or updated items in the bib-file include pmid and doi if they did not previously\n",
    "\n",
    "for index, bib_item in manually_checked.iterrows():\n",
    "    print(f\"Working on {index}/{len(manually_checked)}\")\n",
    "    # Make sure item is manually checked\n",
    "    if \",\" in bib_item['action']:\n",
    "        print(f\"{bib_item['ss_id']} has not been checked yet, make sure only 1 action is mentioned\")\n",
    "        failed_to_find_actions.append(bib_item)\n",
    "        continue\n",
    "        #TODO: we will later work from a dropdown-list rather than a comma separated set of actions so this probably will need updating\n",
    "\n",
    "    # Add new item to diag.bib\n",
    "    elif \"add new item\" in bib_item['action']:\n",
    "       bib_item_text = get_bib_info(diag_bib_readonly, bib_item)\n",
    "\n",
    "       if bib_item_text is not None:\n",
    "           items_to_add += bib_item_text\n",
    "       else:\n",
    "           failed_new_items.append(bib_item)\n",
    "\n",
    "    # Add ss_id to already existing doi in diag.bib\n",
    "    elif \"add ss_id\" in bib_item['action']:\n",
    "        # just store a list of these items for now and we will update the file at the end\n",
    "        items_to_update += [bib_item]\n",
    "        \n",
    "    # Get items to blacklist\n",
    "    elif \"blacklist\" in bib_item['action']:\n",
    "        blacklist_item = get_item_to_blacklist(bib_item)\n",
    "        blacklist_items.append(blacklist_item)\n",
    "\n",
    "    # Get None items\n",
    "    elif 'None' in bib_item['action']:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print('failed to find action')\n",
    "        failed_to_find_actions.append(bib_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Add new bib entries to the diag.bib file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First we use the bib file string, add the completely new bib entries and save it\n",
    "# append the new items to the string\n",
    "diag_bib_readonly += items_to_add  \n",
    "# save the file to disk \n",
    "# TODO : write to correct location\n",
    "diag_bib_path_tmp_new = os.path.join(parent_directory, 'scripts/script_data/diag_ss_tmp_new.bib')\n",
    "with open(diag_bib_path_tmp_new, 'w', encoding=\"utf8\") as bibtex_file:\n",
    "    bibtex_file.write(diag_bib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update existing bib entries with new ss_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Second we re-open the bib file using the read_bibfile method and update existing items with new ss_ids\n",
    "# TODO read from correct location here\n",
    "diag_bib_raw = read_bibfile(None, diag_bib_path_tmp_new)\n",
    "for item_to_update in items_to_update:\n",
    "    [diag_bib_raw, result] = add_ss_id_to_existing_bibkey(diag_bib_raw, item_to_update[\"ss_id\"], item_to_update[\"bibkey\"])\n",
    "    if(result=='Fail'):\n",
    "        failed_updated_items.append(item_to_update)\n",
    "\n",
    "\n",
    "#Note we are not writing the file yet as we will use the same diag_bib_raw and update the citations on it first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update citation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diag_bib_raw_new_cits = update_citation_count(diag_bib_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: update to the correct output path\n",
    "save_to_file(diag_bib_raw_new_cits, None, diag_bib_path_tmp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update the blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Last we update the blacklist (temporarily commented) (what failures can happen here?)\n",
    "blacklist_df = pd.read_csv('./script_data/blacklist.csv')\n",
    "# TODO: fix to correct output location\n",
    "blacklist_out_file = './script_data/blacklist_tmp_updated.csv'\n",
    "# file writing\n",
    "update_blacklist_csv(blacklist_df, blacklist_items, blacklist_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Here we provide a report of rows where we did not know what to do or we failed to do the action\n",
    "print(\"DONE with processing manually checked items\")\n",
    "print('Failures are as follows:')\n",
    "for item in failed_new_items:\n",
    "    print('Failed to add new bib entry ', item['ss_id'])\n",
    "for item in failed_updated_items:\n",
    "    print('Failed to update exiting bib entry with new ss_id', item['bibkey'], item['ss_id'])\n",
    "for item in failed_to_find_actions:\n",
    "    print('Failed to find valid action for item', item['ss_id'], item['action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = items_to_add.count('{yes}')\n",
    "print(f\"Newly added items: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Blacklisted items: {len(blacklist_items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Abra08a 287 21\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Boo09 38 8\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Dana99 1745 1000\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Ginn01a 51 27\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Hu19 12 0\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Kars90 26 7\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Kars91 28 15\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Kars96b 91 39\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Litj17 3681 1000\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Murp10a 45 28\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Niem09c 74 41\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Staa04a 2720 1000\n",
      "adding gscites Tell21 154\n",
      "adding gscites Teuw18 0\n",
      "adding gscites Thag23 2\n",
      "adding gscites Thee20 0\n",
      "adding gscites Thij23 0\n",
      "updating Timp02 from 10 to 11\n",
      "updating Timp10 from 24 to 35\n",
      "updating Trom12 from 3 to 4\n",
      "adding gscites Turn21 4\n",
      "adding gscites Valk19a 16\n",
      "updating Vare05 from 46 to 52\n",
      "updating Veli08d from 4 to 6\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Veli09 62 1\n",
      "updating Veli09a from 2 to 3\n",
      "updating Veli12 from 30 to 34\n",
      "updating Veli13 from 36 to 45\n",
      "updating Velz20 from 10 to 121\n",
      "updating Ven11a from 8 to 12\n",
      "updating Ven13b from 40 to 45\n",
      "updating Ven16a from 15 to 21\n",
      "updating Ven16f from 6 to 7\n",
      "updating Vend17c from 18 to 36\n",
      "adding gscites Vend18 54\n",
      "adding gscites Venh15a 0\n",
      "updating Venh15b from 45 to 72\n",
      "adding gscites Venh15c 13\n",
      "adding gscites Venh16a 11\n",
      "updating Venh17a from 37 to 89\n",
      "updating Venh17b from 59 to 112\n",
      "adding gscites Venh18 112\n",
      "adding gscites Venk21 51\n",
      "adding gscites Venk23 1\n",
      "adding gscites Vent20 2\n",
      "adding gscites Vent21 16\n",
      "adding gscites Vent23 6\n",
      "updating Veta18 from 56 to 205\n",
      "adding gscites Vina22 3\n",
      "adding gscites Vink88 19\n",
      "adding gscites Vlie22 12\n",
      "updating Voge05 from 5 to 6\n",
      "updating Voge07 from 43 to 45\n",
      "updating Vos08 from 103 to 104\n",
      "updating Vos09 from 14 to 15\n",
      "updating Vos13 from 150 to 162\n",
      "adding gscites Vos19 72\n",
      "adding gscites Vos21 5\n",
      "updating Vree17 from 21 to 39\n",
      "updating Vree18 from 11 to 17\n",
      "updating Vree18b from 14 to 24\n",
      "updating Vree18c from 14 to 34\n",
      "updating Vree18d from 25 to 44\n",
      "adding gscites Vug18 0\n",
      "updating Vuka11 from 4 to 6\n",
      "updating Vuka12 from 24 to 28\n",
      "updating Waal15 from 13 to 17\n",
      "updating Wand17 from 57 to 102\n",
      "updating Wand17a from 28 to 54\n",
      "adding gscites Wild21 1\n",
      "adding gscites Wild23a 1\n",
      "adding gscites Wild23b 1\n",
      "adding gscites Wild23c 0\n",
      "updating Wink15a from 51 to 65\n",
      "adding gscites Wink21 18\n",
      "updating Witt11 from 21 to 24\n",
      "updating Witt12 from 31 to 41\n",
      "adding gscites Witt12a 17\n",
      "updating Witt12b from 11 to 14\n",
      "updating Xie20 from 5 to 91\n",
      "adding gscites Xie21 0\n",
      "adding gscites Xie23 2\n",
      "adding gscites Xie23b 0\n",
      "adding gscites Yu20 24\n",
      "updating Zaid18 from 15 to 45\n",
      "updating Zels15 from 19 to 34\n",
      "updating Zels17b from 32 to 51\n",
      "updating Zels18 from 23 to 51\n",
      "updating Zels18a from 22 to 44\n",
      "adding gscites Zels19a 14\n",
      "adding gscites Zhou20 334\n",
      "done updating citations\n"
     ]
    }
   ],
   "source": [
    "diag_bib_raw_new_cits = update_citation_count(diag_bib_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: update to the correct output path\n",
    "save_to_file(diag_bib_raw_new_cits, None, diag_bib_path_tmp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update the blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39 items added to blacklist'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last we update the blacklist (temporarily commented) (what failures can happen here?)\n",
    "blacklist_df = pd.read_csv('./script_data/blacklist.csv')\n",
    "# TODO: fix to correct output location\n",
    "blacklist_out_file = './script_data/blacklist_tmp_updated.csv'\n",
    "# file writing\n",
    "update_blacklist_csv(blacklist_df, blacklist_items, blacklist_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE with processing manually checked items\n",
      "Failures are as follows:\n",
      "Failed to add new bib entry  03ad8d7078805db6fbd4993b881045b462b4e028\n",
      "Failed to add new bib entry  06ee6ed85131848ef70da625806ba480915aa2e0\n",
      "Failed to add new bib entry  0b78520bea8310ff375cf953bbde10082db0eede\n",
      "Failed to add new bib entry  0df7a4f26d57eb58fe628316aa5e84e5ca474ee8\n",
      "Failed to add new bib entry  0ebe8ab65571514718283cd2d8ac7277db3513c5\n",
      "Failed to add new bib entry  0f27fc10d593859a440c6ccf901d5093f67939bd\n",
      "Failed to add new bib entry  17b918178a85cdb670be7521e6cef3b4dbffb16b\n",
      "Failed to add new bib entry  1d2109f8ec43c23db647c4778a5bb5846074e575\n",
      "Failed to add new bib entry  202f393ad41b85acbc59a28e5080d19c9de56988\n",
      "Failed to add new bib entry  233a8c1b929ccbb0f4a31720919e2b9f413a239c\n",
      "Failed to add new bib entry  269e8609dff88d78e8e3c41f81a97199c9add3dd\n",
      "Failed to add new bib entry  2f2182f8e55be5a85c1316cd1b181cd5c85c106c\n",
      "Failed to add new bib entry  32af51ced47419cff26fde66cce602fbab2f238a\n",
      "Failed to add new bib entry  362c510dec0d566d22d5be3af0519fc7eec8bb86\n",
      "Failed to add new bib entry  36fb5c86a92b941f0754cb864e2c4c70b21b7b7d\n",
      "Failed to add new bib entry  5a09637587e694a03f68a4ee1046d31aa97fd0c0\n",
      "Failed to add new bib entry  5cf666b6326b85a31b4e2759031392f0a49351b2\n",
      "Failed to add new bib entry  663336d2c2efa0a4bfe8a4988eb8d1b87e9a7403\n",
      "Failed to add new bib entry  6769e24a1e5a4b5841fd8bcb0b8daa3051b52214\n",
      "Failed to add new bib entry  67f07af40a5c7e2b008509e4e8f61030ce9f85ab\n",
      "Failed to add new bib entry  7593ee0a8242026714b37ba2c2805d1249c82e13\n",
      "Failed to add new bib entry  8706660fbf3110338bc794e354bc3d1c0075d230\n",
      "Failed to add new bib entry  877ce11291735ee26d2a618adde8db726808b107\n",
      "Failed to add new bib entry  926fe98caca0db9cef3d065f90b19ace24dedc76\n",
      "Failed to add new bib entry  952bdfe8a2732d537363028114718edad19bc451\n",
      "Failed to add new bib entry  98cec4020ccc1ef0399b4f866544a30fb550d34c\n",
      "Failed to add new bib entry  99965d464a74d2083ce198156e6b0ca4d043b128\n",
      "Failed to add new bib entry  9ddb2f47695191553a3623ac33eddeb9c7e416cd\n",
      "Failed to add new bib entry  a0f7cee93c06634d1945b614463977548c2b94bd\n",
      "Failed to add new bib entry  af346d53f267840fc87db7ff4f1ff1b97cfe713a\n",
      "Failed to add new bib entry  b8c484519a8970bf4cb07c3c609c41a05365f258\n",
      "Failed to add new bib entry  be95c8bbd8a4297d620b1c2644cf2a898603e355\n",
      "Failed to add new bib entry  c0f6940d1af8063139d99b12f7e451169278ec33\n",
      "Failed to add new bib entry  c705e376f3d2aaf91deb1ca806e838d94a38dda8\n",
      "Failed to add new bib entry  cf46e880665be3dd1b2a81cc1be53f1ea9d64de1\n",
      "Failed to add new bib entry  ddd8894c4281d91727ab1827501cde67f5cc322d\n",
      "Failed to add new bib entry  e1d86927b130950ef8c67caa76364bb336b083d7\n",
      "Failed to add new bib entry  e4729ac7bfdb707e3207b0a91b57a2f907f5351b\n",
      "Failed to add new bib entry  e4dbe4f2c08eeae4cab346e24327eefa65e44191\n",
      "Failed to add new bib entry  e717ffb38990d5da64a82f8b8715bfb56daf9762\n",
      "Failed to add new bib entry  ea9cff43d07c6e1e63dc9d88ff13c8ab7e5380af\n",
      "Failed to add new bib entry  eb6e86cf9697391771d1e2fbad3f49c1448e3411\n",
      "Failed to add new bib entry  efaa4e593dbf76786a33dfd2dff79b77396397cc\n",
      "Failed to add new bib entry  f9166fa0c5c102618d890bdeab63aca74b017c45\n",
      "Failed to find valid action for item 03ef312b3d3e616fd7f0a2f2260c82ad62ed7ef1 [add ss_id, blacklist ss_id, add new item]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Here we provide a report of rows where we did not know what to do or we failed to do the action\n",
    "print(\"DONE with processing manually checked items\")\n",
    "print('Failures are as follows:')\n",
    "for item in failed_new_items:\n",
    "    print('Failed to add new bib entry ', item['ss_id'])\n",
    "for item in failed_updated_items:\n",
    "    print('Failed to update exiting bib entry with new ss_id', item['bibkey'], item['ss_id'])\n",
    "for item in failed_to_find_actions:\n",
    "    print('Failed to find valid action for item', item['ss_id'], item['action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = items_to_add.count('{yes}')\n",
    "print(f\"Newly added items: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Blacklisted items: {len(blacklist_items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
