{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from get_biblatex import GetBiblatex\n",
    "from bib_handling_code.processbib import read_bibfile\n",
    "from bib_handling_code.processbib import save_to_file\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "from semanticscholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' KM: Remove this fn from here. Dre to update GenerateCSVFile with method like this which handles no-doi items\\ndef remove_blacklist_items(df_new_items):\\n    blacklisted_items = pd.read_csv(\"./script_data/blacklist.csv\")\\n    initial_length = len(df_new_items)\\n    df_new_items = df_new_items[~df_new_items[\\'ss_doi\\'].isin(blacklisted_items[\\'doi\\'].unique().tolist())] # remove blacklisted dois\\n    df_new_items = df_new_items[~df_new_items[\\'ss_id\\'].isin(blacklisted_items[\\'ss_id\\'].unique().tolist())] # remove blacklisted dois\\n\\n    print(f\"{initial_length-len(df_new_items)} items removed from newly found items.\")\\n    return df_new_items\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' KM: Remove this fn from here. Dre to update GenerateCSVFile with method like this which handles no-doi items\n",
    "def remove_blacklist_items(df_new_items):\n",
    "    blacklisted_items = pd.read_csv(\"./script_data/blacklist.csv\")\n",
    "    initial_length = len(df_new_items)\n",
    "    df_new_items = df_new_items[~df_new_items['ss_doi'].isin(blacklisted_items['doi'].unique().tolist())] # remove blacklisted dois\n",
    "    df_new_items = df_new_items[~df_new_items['ss_id'].isin(blacklisted_items['ss_id'].unique().tolist())] # remove blacklisted dois\n",
    "\n",
    "    print(f\"{initial_length-len(df_new_items)} items removed from newly found items.\")\n",
    "    return df_new_items\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_item_to_blacklist(item): # item here is a row from the manually checked csv file\n",
    "    #Add item to blacklist.csv\n",
    "    move_to_blacklist = {\n",
    "        'staff_id': item.get('staff_id', None),\n",
    "        'staff_name': item.get('staff_id', None),\n",
    "        'ss_year': item.get('ss_year', None),\n",
    "        'ss_id': item.get('ss_id', None),\n",
    "        'title': item.get('ss_title', None),\n",
    "        'doi': item.get('ss_doi', None),\n",
    "        'Should be in diag.bib': 'no',\n",
    "        'Reason': item.get('Blacklist reason', None)\n",
    "    }\n",
    "\n",
    "    return move_to_blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_blacklist_csv(blacklist_df, blacklist_entries, blacklist_out_file): #blacklist_csv is a df\n",
    "    # Add all items to blacklist.csv\n",
    "    blacklist_df = pd.concat([blacklist_df, pd.DataFrame(blacklist_entries)], ignore_index=True)\n",
    "\n",
    "    # Save blacklist.csv\n",
    "    blacklist_df.to_csv(blacklist_out_file, index=False)\n",
    "    return f\"{len(blacklist_entries)} items added to blacklist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code to get citations from semantic scholar. If there are multiple ss_ids, we should get the number of citations for each of them and sum the two (or more?) values.\n",
    "def get_citations(semantic_scholar_ids):\n",
    "    dict_cits = {}\n",
    "    for ss_id in semantic_scholar_ids:\n",
    "        sch = SemanticScholar()\n",
    "        paper = sch.get_paper(ss_id)\n",
    "        paper_id = paper['paperId']\n",
    "        dict_cits[paper_id] = len(paper['citations'])\n",
    "    return dict_cits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_bib_info(diag_bib_file, item): #diag_bib_file is the file read in as a string, item is row from csv\n",
    "    #Get DOI information\n",
    "\n",
    "    # if no ss_doi exists\n",
    "    if len(str(item['ss_doi']))==0 or str(item['ss_doi'])=='nan':\n",
    "        print('no ss_doi available, I cannot add new bib entry', item['ss_id'])\n",
    "        return None\n",
    "    \n",
    "    # make sure doi is not already in diag.bib\n",
    "    if item['ss_doi'] in diag_bib_file:\n",
    "\n",
    "        start_index = diag_bib_file.find(item['ss_doi'])\n",
    "        end_index = diag_bib_file.find('}', start_index)  # Include the closing brace\n",
    "        matching_item_str = diag_bib_file[start_index:end_index]\n",
    "\n",
    "        print('DOI already exists in bib file. Matching item:', matching_item_str)\n",
    "\n",
    "        if matching_item_str == item['ss_doi']:\n",
    "            print('doi already exists in bib file, I will not add new bib entry', item['ss_doi'], item['ss_id'])\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            print('similar doi already exists in bib file, but new item will be added for ', item['ss_doi'], item['ss_id'])\n",
    "\n",
    "    # Get BibLatex information based on DOI if not in the file\n",
    "    reader = GetBiblatex(doi=item['ss_doi'], diag_bib=diag_bib_file)\n",
    "    bibtext = reader.get_bib_text()\n",
    "\n",
    "    # Return the bibtext if it is not 'empty', otherwise return None\n",
    "    return bibtext if bibtext != 'empty' else None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_ss_id_to_existing_bibkey(diag_bib_raw, ss_id, bibkey):\n",
    "    \n",
    "    #Update bibkey with ss_id\n",
    "    for ind, entry in enumerate(diag_bib_raw):\n",
    "        if entry.type == 'string':\n",
    "            continue\n",
    "\n",
    "        # if we found the relevant key\n",
    "        if bibkey == entry.key:\n",
    "            # if there is already something in all_ss_ids\n",
    "            if 'all_ss_ids' in entry.fields.keys():\n",
    "                if not entry.fields['all_ss_ids'] == '{' + str(ss_id) + '}': # this should never happen, right? (from Keelin!)\n",
    "                    previous = literal_eval(entry.fields['all_ss_ids'].strip('{}'))\n",
    "                    new = ss_id\n",
    "                    combined = list(set(previous) | set([new]))\n",
    "                    # update the entry\n",
    "                    entry.fields['all_ss_ids'] = '{' + str(combined) + '}'\n",
    "            # if there is no ss_id here yet just add this single one\n",
    "            else:   \n",
    "                    entry.fields['all_ss_ids'] = '{' + str(ss_id) + '}'\n",
    "            print(str(ss_id), 'added to diag_bib_raw')\n",
    "            return [diag_bib_raw, 'Success']\n",
    "        \n",
    "    # if we haven't returned by now then we failed to update \n",
    "    print('failed to add ss_id to diag.bib', str(ss_id), str(bibkey))\n",
    "    return [diag_bib_raw, 'Fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_citation_count(diag_bib_raw):\n",
    "    num_entries = len(diag_bib_raw)\n",
    "    for ind, entry in enumerate(diag_bib_raw):\n",
    "        # print('checking citations', ind, 'of', num_entries)\n",
    "        flag=0\n",
    "        if entry.type == 'string':\n",
    "            continue\n",
    "        if 'all_ss_ids' in entry.fields:\n",
    "            all_ss_ids = []\n",
    "            ss_ids = entry.fields['all_ss_ids'].translate(str.maketrans('', '', string.punctuation)).split(' ')\n",
    "            if len(ss_ids) > 1:\n",
    "                all_ss_ids.extend(ss_ids)\n",
    "            else:\n",
    "                all_ss_ids.append(ss_ids[0])\n",
    "            dict_cits = get_citations(all_ss_ids)\n",
    "            n_cits = 0\n",
    "            for key in dict_cits.keys():\n",
    "                n_cits += dict_cits[key]\n",
    "            # TODO: is it correct logic to use this field name or should we make a new one?\n",
    "            if 'gscites' in entry.fields:\n",
    "                # only update if we are increasing the number of citations!!!\n",
    "                previous_cits = int(entry.fields['gscites'].strip('{}'))\n",
    "                if n_cits > previous_cits:\n",
    "                    print('updating', entry.key, 'from', previous_cits, 'to', n_cits)\n",
    "                    entry.fields['gscites'] = '{' + str(n_cits) + '}'\n",
    "                elif (previous_cits > (1.5 * n_cits)) and (previous_cits - n_cits > 10):\n",
    "                    print('warning: num citations calculated for this bibkey is much lower than previously suggested....', entry.key, previous_cits, n_cits)\n",
    "            else:\n",
    "                print('adding gscites', entry.key, n_cits)\n",
    "                entry.fields['gscites'] = '{' + str(n_cits) + '}'\n",
    "    print('done updating citations')\n",
    "    return diag_bib_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load manually checked csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load manually_checked\n",
    "manually_checked = pd.read_excel(\"./script_data/manual_check_20231018.xlsx\")\n",
    "# manually_checked = remove_blacklist_items(manually_checked)     # This should be done before actually manually checking\n",
    "\n",
    "# load bib file just for reading at this point\n",
    "#TODO: in the end when this script is routine this should just read the live diag.bib\n",
    "cwd = os.getcwd()\n",
    "parent_directory = os.path.dirname(cwd)\n",
    "diag_bib_path = os.path.join(parent_directory, 'scripts/script_data/diag_ss.bib')\n",
    "with open(diag_bib_path, 'r', encoding=\"utf8\") as readonly_bib_file:\n",
    "    diag_bib_readonly = readonly_bib_file.read()\n",
    "    \n",
    "# POTENTIAL TO-DO CREATE ACTION MAPPINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0/280: 10.1016/j.media.2022.102605\n",
      "Working on 1/280: 10.1177/0271678X18756218\n",
      "Working on 2/280: 10.1016/j.ejso.2022.11.378\n",
      "Working on 3/280: nan\n",
      "Working on 4/280: 10.1038/s41585-020-0324-x\n",
      "Working on 5/280: nan\n",
      "Working on 6/280: 10.1148/radiol.2019182666\n",
      "Working on 7/280: nan\n",
      "Working on 8/280: 10.1016/j.ejso.2020.04.038\n",
      "Working on 9/280: 10.23698/AIDA/BRLN\n",
      "Working on 10/280: 10.1093/neuonc/now292.004\n",
      "Working on 11/280: 10.1093/cid/ciac623\n",
      "Working on 12/280: 10.1186/s13058-018-0961-7\n",
      "Working on 13/280: 10.1002/mp.12408\n",
      "Working on 14/280: 10.1148/radiol.230318\n",
      "Working on 15/280: 10.48550/arXiv.2112.05151\n",
      "Working on 16/280: 10.1093/rheumatology/keab835\n",
      "Working on 17/280: 10.1001/jamaophthalmol.2021.1407\n",
      "Working on 18/280: 10.1016/j.resuscitation.2022.03.025\n",
      "Working on 19/280: 10.1148/radiol.210832\n",
      "Working on 20/280: 10.1016/j.ebiom.2022.104427\n",
      "Working on 21/280: 10.1016/j.ejrad.2021.109894\n",
      "Working on 22/280: 10.1007/978-3-319-24574-4_13\n",
      "Working on 23/280: 10.1118/1.4937787\n",
      "Working on 24/280: nan\n",
      "Working on 25/280: nan\n",
      "Working on 26/280: nan\n",
      "Working on 27/280: 10.48550/arXiv.2212.08568\n",
      "Working on 28/280: nan\n",
      "Working on 29/280: 10.3390/cancers14122992\n",
      "Working on 30/280: 10.1002/jum.16192\n",
      "Working on 31/280: 10.1109/TMI.2016.2553401\n",
      "Working on 32/280: nan\n",
      "Working on 33/280: 10.1055/a-1543-6156\n",
      "Working on 34/280: nan\n",
      "Working on 35/280: nan\n",
      "Working on 36/280: 10.1002/hipo.23039\n",
      "Working on 37/280: 10.48550/arXiv.1803.05471\n",
      "Working on 38/280: 10.1038/s41374-019-0275-0\n",
      "Working on 39/280: 10.1117/12.2564155\n",
      "Working on 40/280: 10.48550/arXiv.1908.06037\n",
      "Working on 41/280: 10.3390/diagnostics13203210\n",
      "Working on 42/280: 10.1016/S1470-2045(19)30739-9\n",
      "Working on 43/280: 10.1016/j.media.2023.102935\n",
      "Working on 44/280: 10.1183/2312508X.10003015\n",
      "Working on 45/280: 10.48550/arXiv.2109.07892\n",
      "Working on 46/280: 10.1007/978-3-319-47157-0_37\n",
      "Working on 47/280: 10.1148/radiol.2018182318\n",
      "Working on 48/280: 10.1007/978-3-031-33842-7_23\n",
      "Working on 49/280: 10.1117/12.2081909\n",
      "Working on 50/280: 10.1016/j.neuroimage.2022.119725\n",
      "Working on 51/280: 10.1118/1.4917481\n",
      "Working on 52/280: 10.1002/mrm.26667\n",
      "Working on 53/280: 10.5244/C.30.71\n",
      "Working on 54/280: 10.1007/978-3-030-62469-9\n",
      "Working on 55/280: 10.1016/j.jval.2017.04.012\n",
      "Working on 56/280: 10.1161/STROKEAHA.118.020980\n",
      "Working on 57/280: 10.1016/j.jpi.2022.100026\n",
      "Working on 58/280: nan\n",
      "Working on 59/280: nan\n",
      "Working on 60/280: 10.1016/j.resuscitation.2022.10.002\n",
      "Working on 61/280: 10.1186/s13000-021-01136-w\n",
      "Working on 62/280: 10.1097/MCP.0000000000000355\n",
      "Working on 63/280: nan\n",
      "Working on 64/280: 10.1117/1.JMI.8.2.024501\n",
      "Working on 65/280: 10.48550/arXiv.2206.01653\n",
      "Working on 66/280: 10.1016/j.ultrasmedbio.2021.12.006\n",
      "Working on 67/280: 10.1117/12.2293717\n",
      "Working on 68/280: 10.1016/j.chest.2020.11.027\n",
      "Working on 69/280: 10.1242/dmm.046342\n",
      "Working on 70/280: 10.1186/s12874-021-01243-8\n",
      "Working on 71/280: 10.1201/B18191-23\n",
      "Working on 72/280: nan\n",
      "Working on 73/280: 10.1002/rth2.12422\n",
      "Working on 74/280: 10.3174/ajnr.A4162\n",
      "Working on 75/280: 10.48550/arXiv.2302.03116\n",
      "Working on 76/280: 10.1201/B18191-7\n",
      "Working on 77/280: 10.1364/BOE.9.001545\n",
      "Working on 78/280: 10.1007/s00330-021-08519-z\n",
      "Working on 79/280: 10.1167/iovs.15-18571\n",
      "Working on 80/280: 10.1109/ISBI.2018.8363641\n",
      "Working on 81/280: 10.1016/j.media.2023.102881\n",
      "Working on 82/280: 10.5120/IJCA2017913091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 83/280: 10.1038/s41523-021-00378-7\n",
      "Working on 84/280: 10.1158/2326-6066.CIR-20-0741\n",
      "Working on 85/280: 10.1109/CBMS.2017.29\n",
      "Working on 86/280: 10.1055/s-0040-1713119\n",
      "Working on 87/280: 10.1136/ijgc-2022-esgo.790\n",
      "Working on 88/280: 10.1148/radiol.2017162894\n",
      "Working on 89/280: nan\n",
      "Working on 90/280: 10.1109/TPAMI.2019.2936841\n",
      "Working on 91/280: 10.1117/12.2317937\n",
      "Working on 92/280: 10.5220/0011669000003414\n",
      "Working on 93/280: 10.3390/cancers14133260\n",
      "Working on 94/280: nan\n",
      "Working on 95/280: 10.1007/978-3-319-67534-3_2\n",
      "Working on 96/280: 10.1177/10935266211059809\n",
      "Working on 97/280: 10.1117/12.2564179\n",
      "Working on 98/280: 10.1148/radiol.2015142856\n",
      "Working on 99/280: 10.1117/12.2209613\n",
      "Working on 100/280: 10.1007/978-3-030-00949-6_14\n",
      "Working on 101/280: 10.1007/s10549-022-06600-9\n",
      "Working on 102/280: 10.1007/s00117-020-00675-5\n",
      "Working on 103/280: 10.1148/radiol.2020204038\n",
      "Working on 104/280: 10.3174/ajnr.A7028\n",
      "Working on 105/280: 10.3389/fnins.2022.919186\n",
      "Working on 106/280: nan\n",
      "Working on 107/280: 10.5281/ZENODO.4008954\n",
      "Working on 108/280: 10.48550/arXiv.2201.04532\n",
      "Working on 109/280: 10.1097/RTI.0000000000000255\n",
      "Working on 110/280: 10.48550/arXiv.2106.13150\n",
      "Working on 111/280: 10.6084/M9.FIGSHARE.2077399.V1\n",
      "Working on 112/280: 10.3390/diagnostics11060959\n",
      "Working on 113/280: nan\n",
      "Working on 114/280: 10.1371/journal.pone.0290118\n",
      "Working on 115/280: nan\n",
      "Working on 116/280: 10.1186/s13256-023-04097-4\n",
      "Working on 117/280: nan\n",
      "Working on 118/280: 10.1001/jamanetworkopen.2022.12964\n",
      "Working on 119/280: 10.1002/mrm.29371\n",
      "Working on 120/280: 10.3390/jpm11070663\n",
      "Working on 121/280: 10.48550/arXiv.2305.05984\n",
      "Working on 122/280: 10.1038/s41388-023-02797-1\n",
      "Working on 123/280: 10.1158/1538-7445.sabcs22-p2-11-34\n",
      "Working on 124/280: nan\n",
      "Working on 125/280: 10.5281/ZENODO.3715938\n",
      "Working on 126/280: 10.1016/j.ejca.2022.09.018\n",
      "Working on 127/280: 10.1016/j.ejrad.2017.01.021\n",
      "Working on 128/280: nan\n",
      "Working on 129/280: 10.1016/j.media.2022.102605\n",
      "Working on 130/280: 10.1093/cid/ciz1008\n",
      "Working on 131/280: nan\n",
      "Working on 132/280: 10.1148/radiol.230275\n",
      "Working on 133/280: 10.1038/s41523-021-00378-7\n",
      "Working on 134/280: 10.48550/arXiv.1910.10470\n",
      "Working on 135/280: 10.1016/j.ejrad.2017.01.020\n",
      "Working on 136/280: 10.1259/bjrcr.20200015\n",
      "Working on 137/280: 10.1093/annonc/mdx393.087\n",
      "Working on 138/280: 10.1007/s00330-019-06020-2\n",
      "Working on 139/280: 10.1097/GOX.0000000000004495\n",
      "Working on 140/280: 10.1117/12.2211640\n",
      "Working on 141/280: 10.1007/s00330-021-08217-w\n",
      "Working on 142/280: 10.1016/j.ophtha.2018.02.005\n",
      "Working on 143/280: 10.1007/s10549-020-05814-z\n",
      "Working on 144/280: 10.1002/ijc.32353\n",
      "Working on 145/280: nan\n",
      "Working on 146/280: 10.2214/AJR.21.25903\n",
      "Working on 147/280: 10.1109/JBHI.2023.3323582\n",
      "Working on 148/280: 10.1007/s00259-021-05375-3\n",
      "Working on 149/280: 10.1016/j.breast.2016.06.020\n",
      "Working on 150/280: 10.1038/s41467-019-08563-w\n",
      "Working on 151/280: 10.48550/arXiv.2103.16515\n",
      "Working on 152/280: 10.1007/978-3-030-32245-8_16\n",
      "Working on 153/280: 10.48550/arXiv.2011.07952\n",
      "Working on 154/280: 10.1111/pin.13309\n",
      "Working on 155/280: 10.3389/fcvm.2022.981901\n",
      "Working on 156/280: 10.1002/mp.13342\n",
      "Working on 157/280: 10.1117/12.2318016\n",
      "Working on 158/280: 10.1148/radiol.2021203427\n",
      "Working on 159/280: 10.1007/s00330-022-08702-w\n",
      "Working on 160/280: 10.1371/journal.pgph.0001799\n",
      "Working on 161/280: 10.3389/fnimg.2022.977491\n",
      "Working on 162/280: nan\n",
      "Working on 163/280: 10.5281/ZENODO.3715001\n",
      "Working on 164/280: 10.1183/13993003.02183-2017\n",
      "Working on 165/280: nan\n",
      "Working on 166/280: 10.1007/978-3-319-30355-0_7\n",
      "Working on 167/280: nan\n",
      "Working on 168/280: 10.1007/978-3-030-67194-5_8\n",
      "Working on 169/280: 10.1016/j.ejrad.2021.109626\n",
      "Working on 170/280: 10.1016/j.ejrad.2021.109626\n",
      "Working on 171/280: 10.1007/978-3-030-97281-3_21\n",
      "Working on 172/280: 10.48550/arXiv.2012.06318\n",
      "Working on 173/280: 10.1007/s00234-021-02839-z\n",
      "Working on 174/280: 10.1016/j.ejca.2021.11.005\n",
      "Working on 175/280: 10.48550/arXiv.2106.13150\n",
      "Working on 176/280: 10.22443/RMS.EMC2020.286\n",
      "Working on 177/280: 10.1016/j.euf.2016.07.003\n",
      "Working on 178/280: 10.48550/arXiv.2212.13439\n",
      "Working on 179/280: 10.48550/arXiv.2006.06356\n",
      "Working on 180/280: 10.5281/ZENODO.3715652\n",
      "Working on 181/280: nan\n",
      "Working on 182/280: nan\n",
      "Working on 183/280: 10.1007/978-3-319-68548-9_27\n",
      "Working on 184/280: 10.2214/AJR.18.20786\n",
      "Working on 185/280: 10.1117/12.2254160\n",
      "Working on 186/280: 10.1007/s00330-021-08035-0\n",
      "Working on 187/280: 10.48550/arXiv.2306.10484\n",
      "Working on 188/280: 10.1016/j.chest.2020.04.003\n",
      "Working on 189/280: 10.1038/s41585-023-00748-9\n",
      "Working on 190/280: nan\n",
      "Working on 191/280: 10.1007/s00330-018-5740-4\n",
      "Working on 192/280: nan\n",
      "Working on 193/280: 10.3390/jof8090925\n",
      "Working on 194/280: 10.1148/radiol.210948\n",
      "Working on 195/280: 10.1117/12.2611423\n",
      "Working on 196/280: 10.1109/JBHI.2020.3039741\n",
      "Working on 197/280: 10.1007/174_2015_1080\n",
      "Working on 198/280: 10.1109/CVPR52729.2023.01911\n",
      "Working on 199/280: nan\n",
      "Working on 200/280: 10.1371/journal.pcbi.1004823\n",
      "Working on 201/280: 10.1038/s41523-021-00346-1\n",
      "Working on 202/280: 10.48550/arXiv.2301.06304\n",
      "Working on 203/280: 10.1212/WNL.0000000000008364\n",
      "Working on 204/280: nan\n",
      "Working on 205/280: 10.1007/s00261-021-03207-4\n",
      "Working on 206/280: 10.1016/j.ejrad.2018.01.015\n",
      "Working on 207/280: 10.1109/ACCESS.2022.3141021\n",
      "Working on 208/280: 10.48550/arXiv.1703.07715\n",
      "Working on 209/280: 10.4103/2153-3539.255259\n",
      "Working on 210/280: 10.1007/978-3-030-00949-6\n",
      "Working on 211/280: 10.1136/ijgc-2021-esgo.533\n",
      "Working on 212/280: 10.48550/arXiv.2309.03383\n",
      "Working on 213/280: 10.1002/jmri.28935\n",
      "Working on 214/280: 10.1101/2022.05.17.492245\n",
      "Working on 215/280: 10.3390/cancers15092656\n",
      "Working on 216/280: nan\n",
      "Working on 217/280: 10.1148/radiol.2017161659\n",
      "Working on 218/280: 10.1109/ISBI52829.2022.9761453\n",
      "Working on 219/280: 10.1016/j.neo.2014.12.011\n",
      "Working on 220/280: 10.1186/s13063-020-04595-6\n",
      "Working on 221/280: nan\n",
      "Working on 222/280: 10.1007/s00330-020-07598-8\n",
      "Working on 223/280: nan\n",
      "Working on 224/280: nan\n",
      "Working on 225/280: nan\n",
      "Working on 226/280: 10.4103/jpi.jpi_64_19\n",
      "Working on 227/280: 10.1371/journal.pgph.0001488\n",
      "Working on 228/280: nan\n",
      "Working on 229/280: 10.48550/arXiv.1812.00964\n",
      "Working on 230/280: 10.1016/j.parkreldis.2018.11.010\n",
      "Working on 231/280: 10.1016/j.media.2021.102061\n",
      "Working on 232/280: 10.1007/s00428-021-03059-9\n",
      "Working on 233/280: 10.1177/0284185114531754\n",
      "Working on 234/280: 10.1117/12.2318054\n",
      "Working on 235/280: 10.3390/diagnostics12020436\n",
      "Working on 236/280: nan\n",
      "Working on 237/280: 10.1177/02841851221076324\n",
      "Working on 238/280: 10.1016/j.media.2019.101544\n",
      "Working on 239/280: 10.1183/13993003.01359-2018\n",
      "Working on 240/280: 10.1117/12.2586357\n",
      "Working on 241/280: 10.3390/diagnostics12071690\n",
      "Working on 242/280: 10.48550/arXiv.2001.09193\n",
      "Working on 243/280: nan\n",
      "Working on 244/280: 10.1016/j.ophtha.2017.05.035\n",
      "Working on 245/280: 10.1111/his.14902\n",
      "Working on 246/280: 10.1093/cercor/bhac154\n",
      "Working on 247/280: 10.1016/j.ophtha.2018.09.045\n",
      "Working on 248/280: nan\n",
      "Working on 249/280: 10.1007/s00330-021-07798-w\n",
      "Working on 250/280: nan\n",
      "failed to find action\n",
      "Working on 251/280: nan\n",
      "Working on 252/280: 10.1148/radiol.2021203633\n",
      "Working on 253/280: nan\n",
      "Working on 254/280: 10.1007/978-3-030-00949-6\n",
      "Working on 255/280: 10.1016/j.chest.2021.01.084\n",
      "Working on 256/280: 10.1088/1361-6560/aa628f\n",
      "Working on 257/280: 10.1101/2022.09.02.22279476\n",
      "Working on 258/280: 10.1055/B000000232\n",
      "Working on 259/280: 10.1183/16000617.0079-2018\n",
      "Working on 260/280: nan\n",
      "Working on 261/280: nan\n",
      "Working on 262/280: 10.1007/s00330-021-07992-w\n",
      "Working on 263/280: 10.48550/arXiv.2012.04974\n",
      "Working on 264/280: 10.52294/001c.87638\n",
      "Working on 265/280: 10.1117/12.2512054\n",
      "Working on 266/280: nan\n",
      "Working on 267/280: 10.5281/ZENODO.4573968\n",
      "Working on 268/280: 10.1016/j.ophtha.2020.02.009\n",
      "Working on 269/280: 10.1136/thoraxjnl-2018-211891\n",
      "Working on 270/280: nan\n",
      "Working on 271/280: 10.1117/12.2512818\n",
      "Working on 272/280: 10.1101/158014\n",
      "Working on 273/280: 10.1016/j.ophtha.2018.03.026\n",
      "Working on 274/280: 10.1007/s00428-021-03213-3\n",
      "Working on 275/280: 10.1093/cid/ciaa1855\n",
      "Working on 276/280: nan\n",
      "Working on 277/280: 10.1186/s13058-022-01541-z\n",
      "Working on 278/280: 10.21037/TLCR-20-924\n",
      "Working on 279/280: 10.1117/12.2318069\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all items in the manually checked csv\n",
    "blacklist_items = []\n",
    "items_to_add = ''\n",
    "items_to_update = []\n",
    "\n",
    "failed_new_items = []\n",
    "failed_updated_items = []\n",
    "failed_to_find_actions = []\n",
    "\n",
    "#TODO: Make sure new items or updated items in the bib-file include pmid and doi if they did not previously\n",
    "#manually_checked = manually_checked[manually_checked.ss_id == 'c859bd469080b82dc14db62e78d65ef5b5ffa686']\n",
    "\n",
    "for index, bib_item in manually_checked.iterrows():\n",
    "    print(f\"Working on {index}/{len(manually_checked)}: {bib_item['ss_doi']}\")\n",
    "    # Make sure item is manually checked\n",
    "    if \",\" in bib_item['action']:\n",
    "        print(f\"{bib_item['ss_id']} has not been checked yet, make sure only 1 action is mentioned\")\n",
    "        failed_to_find_actions.append(bib_item)\n",
    "        continue\n",
    "        #TODO: we will later work from a dropdown-list rather than a comma separated set of actions so this probably will need updating\n",
    "\n",
    "    # Add new item to diag.bib\n",
    "    elif \"add new item\" in bib_item['action']:\n",
    "       bib_item_text = get_bib_info(diag_bib_readonly, bib_item)\n",
    "\n",
    "       if bib_item_text is not None:\n",
    "           items_to_add += bib_item_text\n",
    "       else:\n",
    "           failed_new_items.append(bib_item)\n",
    "\n",
    "    # Add ss_id to already existing doi in diag.bib\n",
    "    elif \"add ss_id\" in bib_item['action']:\n",
    "        # just store a list of these items for now and we will update the file at the end\n",
    "        items_to_update += [bib_item]\n",
    "        \n",
    "    # Get items to blacklist\n",
    "    elif \"blacklist\" in bib_item['action']:\n",
    "        blacklist_item = get_item_to_blacklist(bib_item)\n",
    "        blacklist_items.append(blacklist_item)\n",
    "\n",
    "    # Get None items\n",
    "    elif 'None' in bib_item['action']:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print('failed to find action')\n",
    "        failed_to_find_actions.append(bib_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Add new bib entries to the diag.bib file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First we use the bib file string, add the completely new bib entries and save it\n",
    "# append the new items to the string\n",
    "diag_bib_readonly += items_to_add  \n",
    "# save the file to disk \n",
    "# TODO : write to correct location\n",
    "diag_bib_path_tmp_new = os.path.join(parent_directory, 'scripts/script_data/diag_ss_tmp_new.bib')\n",
    "with open(diag_bib_path_tmp_new, 'w', encoding=\"utf8\") as bibtex_file:\n",
    "    bibtex_file.write(diag_bib_readonly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update existing bib entries with new ss_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03ef312b3d3e616fd7f0a2f2260c82ad62ed7ef1 added to diag_bib_raw\n",
      "14198a817d2c0a800bfb0a0a36baf5097fe22054 added to diag_bib_raw\n",
      "14fcdfdd2b15f6fec9b9e7b1b4189e43281273d8 added to diag_bib_raw\n",
      "1d2109f8ec43c23db647c4778a5bb5846074e575 added to diag_bib_raw\n",
      "1d3c69edf9e573412de0c758b3db1b8f9996f2c1 added to diag_bib_raw\n",
      "1dddfd64c4a40269d63014b21ed3ed436f38b98b added to diag_bib_raw\n",
      "2125835bf1c4fd0646b5dd50855d647044c07658 added to diag_bib_raw\n",
      "24b8cee45431f633d2fa6e3c05670f62b1e41e7e added to diag_bib_raw\n",
      "2723ce1686eea776df179e362cd9a8b8e2bb7ff1 added to diag_bib_raw\n",
      "2a7dafe1287670068300ff77401923f7e151b9f4 added to diag_bib_raw\n",
      "2e571724830cb8ca6e8dbe9cc1f92fdcfc517ec4 added to diag_bib_raw\n",
      "2fe9af8a6b41fc9db41e76621f037aac453ab433 added to diag_bib_raw\n",
      "34559bb0d95c5166625945eef9b53b21a30838fa added to diag_bib_raw\n",
      "349ca29f588b9c785085da7147a4b58df032a8bf added to diag_bib_raw\n",
      "374f4a7676183c95f901e655f2caf170cdd9ec9d added to diag_bib_raw\n",
      "37e383517c34818ad049af0aa763ad5906e9f51a added to diag_bib_raw\n",
      "42315be6452636824d9004d5d8aa2fe8924494a2 added to diag_bib_raw\n",
      "44454c9090606d0332272ff38df6c87eac15f5f7 added to diag_bib_raw\n",
      "4f016eb85905d24ce82adeee28bdf66b46870c9e added to diag_bib_raw\n",
      "589037d110f74defb9cac3b42a370f7b044a1c4b added to diag_bib_raw\n",
      "5a09637587e694a03f68a4ee1046d31aa97fd0c0 added to diag_bib_raw\n",
      "5cf666b6326b85a31b4e2759031392f0a49351b2 added to diag_bib_raw\n",
      "6ab3424f10b236c992d823568dfca2075e2ad46e added to diag_bib_raw\n",
      "6d1ea27b41023f9add67e2c8c4dcbc7866ae640b added to diag_bib_raw\n",
      "740d5d34fcd714870ddf0073fd8956db023319f0 added to diag_bib_raw\n",
      "7593ee0a8242026714b37ba2c2805d1249c82e13 added to diag_bib_raw\n",
      "80af090645088134f058db53a708b7092dd28786 added to diag_bib_raw\n",
      "862ef80662ea6cd8e646df642abc3fd343263191 added to diag_bib_raw\n",
      "8706660fbf3110338bc794e354bc3d1c0075d230 added to diag_bib_raw\n",
      "877ce11291735ee26d2a618adde8db726808b107 added to diag_bib_raw\n",
      "8a039fe22daf6f65a32ba02035f45a8c67b48339 added to diag_bib_raw\n",
      "8b92dcfb8d8b92314d63de92852a28880a81f4ea added to diag_bib_raw\n",
      "8c9301f67b46bbac884c588232ebca05dd3ba8ae added to diag_bib_raw\n",
      "8ce6b544554a79e077e5fc52f55ba8234ce606d4 added to diag_bib_raw\n",
      "926fe98caca0db9cef3d065f90b19ace24dedc76 added to diag_bib_raw\n",
      "952bdfe8a2732d537363028114718edad19bc451 added to diag_bib_raw\n",
      "974e8ae2bfa594ba157da2794a3254d12eb7bf26 added to diag_bib_raw\n",
      "99965d464a74d2083ce198156e6b0ca4d043b128 added to diag_bib_raw\n",
      "9a749224752f101c29f177f941bb5c967855db27 added to diag_bib_raw\n",
      "a11db962a303ac6cb6ff4216659189c2ed378c21 added to diag_bib_raw\n",
      "a1b328c04b54decb972bf83458f9df4ab5608af3 added to diag_bib_raw\n",
      "a46c8e13b227cfc9d208915fdc79a6aff9fc58ea added to diag_bib_raw\n",
      "ab07173f4e352d07f48eddd67136e8e33573aecf added to diag_bib_raw\n",
      "b8c484519a8970bf4cb07c3c609c41a05365f258 added to diag_bib_raw\n",
      "c21f83954bab86611134478ab918399813c10313 added to diag_bib_raw\n",
      "c37197020c3e86415f814d66e49de6f11d7cdbf1 added to diag_bib_raw\n",
      "cb774bdd19514d661ab76d0d30d158e4e74a859f added to diag_bib_raw\n",
      "cc7605f2b7e61723f12839fabc1066da0cc8744b added to diag_bib_raw\n",
      "ce40309e3a7d5319ea2337c2d44f75bca3761e78 added to diag_bib_raw\n",
      "cf46e880665be3dd1b2a81cc1be53f1ea9d64de1 added to diag_bib_raw\n",
      "d6e975989345b69f539dbb8f22cb3437f5cc5039 added to diag_bib_raw\n",
      "ddd8894c4281d91727ab1827501cde67f5cc322d added to diag_bib_raw\n",
      "e4db301a185bccd105017fb66e3f9e2adf876495 added to diag_bib_raw\n",
      "e844b6b027c94468de8a607497f95a3771b7d48b added to diag_bib_raw\n",
      "ed8cbf0b3373cf1dcadd2718dfd3daf6fbe068f3 added to diag_bib_raw\n",
      "ee17bc918583166ad08ff307be5f77cb130486ea added to diag_bib_raw\n",
      "f131ef217543d179269018950bf3b6ba2b30f3b1 added to diag_bib_raw\n",
      "f643f4a927cf65f1ec66231ae76d3bc1736a67d3 added to diag_bib_raw\n",
      "fc12f80e0fe56243c26f628d311577507f34b39c added to diag_bib_raw\n"
     ]
    }
   ],
   "source": [
    "# Second we re-open the bib file using the read_bibfile method and update existing items with new ss_ids\n",
    "# TODO read from correct location here\n",
    "diag_bib_raw = read_bibfile(None, diag_bib_path_tmp_new)\n",
    "for item_to_update in items_to_update:\n",
    "    [diag_bib_raw, result] = add_ss_id_to_existing_bibkey(diag_bib_raw, item_to_update[\"ss_id\"], item_to_update[\"bibkey\"])\n",
    "    if(result=='Fail'):\n",
    "        failed_updated_items.append(item_to_update)\n",
    "\n",
    "\n",
    "#Note we are not writing the file yet as we will use the same diag_bib_raw and update the citations on it first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update citation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Abra08a 287 24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\scripts\\SEMANTIC_SCHOLAR5_update_bib_file.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m diag_bib_raw_new_cits \u001b[39m=\u001b[39m update_citation_count(diag_bib_raw)\n",
      "\u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\scripts\\SEMANTIC_SCHOLAR5_update_bib_file.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     all_ss_ids\u001b[39m.\u001b[39mappend(ss_ids[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m dict_cits \u001b[39m=\u001b[39m get_citations(all_ss_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m n_cits \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dict_cits\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\scripts\\SEMANTIC_SCHOLAR5_update_bib_file.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m ss_id \u001b[39min\u001b[39;00m semantic_scholar_ids:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     sch \u001b[39m=\u001b[39m SemanticScholar()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     paper \u001b[39m=\u001b[39m sch\u001b[39m.\u001b[39;49mget_paper(ss_id)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     paper_id \u001b[39m=\u001b[39m paper[\u001b[39m'\u001b[39m\u001b[39mpaperId\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/z801166/Documents/WebTeam/Literature/scripts/SEMANTIC_SCHOLAR5_update_bib_file.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     dict_cits[paper_id] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(paper[\u001b[39m'\u001b[39m\u001b[39mcitations\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\semanticscholar\\SemanticScholar.py:104\u001b[0m, in \u001b[0;36mSemanticScholar.get_paper\u001b[1;34m(self, paper_id, fields)\u001b[0m\n\u001b[0;32m    101\u001b[0m fields \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(fields)\n\u001b[0;32m    102\u001b[0m parameters \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m&fields=\u001b[39m\u001b[39m{\u001b[39;00mfields\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 104\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requester\u001b[39m.\u001b[39;49mget_data(url, parameters, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth_header)\n\u001b[0;32m    105\u001b[0m paper \u001b[39m=\u001b[39m Paper(data)\n\u001b[0;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m paper\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\semanticscholar\\ApiRequester.py:63\u001b[0m, in \u001b[0;36mApiRequester.get_data\u001b[1;34m(self, url, parameters, headers, payload)\u001b[0m\n\u001b[0;32m     61\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPOST\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m payload \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     62\u001b[0m payload \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(payload) \u001b[39mif\u001b[39;00m payload \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m     64\u001b[0m     method, url, timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout, headers\u001b[39m=\u001b[39;49mheaders, data\u001b[39m=\u001b[39;49mpayload)\n\u001b[0;32m     66\u001b[0m data \u001b[39m=\u001b[39m {}\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m r\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[39m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[39m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[39m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    807\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\z801166\\Documents\\WebTeam\\Literature\\.venv\\lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "diag_bib_raw_new_cits = update_citation_count(diag_bib_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: update to the correct output path\n",
    "save_to_file(diag_bib_raw_new_cits, None, diag_bib_path_tmp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update the blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'43 items added to blacklist'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last we update the blacklist (temporarily commented) (what failures can happen here?)\n",
    "blacklist_df = pd.read_csv('./script_data/blacklist.csv')\n",
    "# TODO: fix to correct output location\n",
    "blacklist_out_file = './script_data/blacklist_tmp_updated.csv'\n",
    "# file writing\n",
    "update_blacklist_csv(blacklist_df, blacklist_items, blacklist_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE with processing manually checked items\n",
      "Failures are as follows:\n",
      "Failed to find valid action for item e4729ac7bfdb707e3207b0a91b57a2f907f5351b [add manually]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Here we provide a report of rows where we did not know what to do or we failed to do the action\n",
    "print(\"DONE with processing manually checked items\")\n",
    "print('Failures are as follows:')\n",
    "for item in failed_new_items:\n",
    "    print('Failed to add new bib entry ', item['ss_id'])\n",
    "for item in failed_updated_items:\n",
    "    print('Failed to update exiting bib entry with new ss_id', item['bibkey'], item['ss_id'])\n",
    "for item in failed_to_find_actions:\n",
    "    print('Failed to find valid action for item', item['ss_id'], item['action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blacklisted items: 43\n",
      "Updated items: 59\n",
      "Newly added items: 173\n",
      "Items with action None: 4\n",
      "total processed items: 280\n",
      "amount of items in manual checkfile: 280\n"
     ]
    }
   ],
   "source": [
    "print(f\"Blacklisted items: {len(blacklist_items)}\")\n",
    "print(f\"Updated items: {len(items_to_update)}\")\n",
    "print(f\"Newly added items: {items_to_add.count('{yes}')}\")\n",
    "import numpy as np\n",
    "count_action_none = np.sum(np.fromiter(('none' in str(action).lower() for action in manually_checked['action']), dtype=bool))\n",
    "print(f\"Items with action None: {count_action_none}\")\n",
    "\n",
    "\n",
    "print(f\"total processed items: {len(blacklist_items) + len(items_to_update) + items_to_add.count('{yes}') + len(failed_new_items) + len(failed_updated_items) + len(failed_to_find_actions) + count_action_none}\")\n",
    "print(f\"amount of items in manual checkfile: {manually_checked.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Abra08a 287 21\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Boo09 38 8\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Dana99 1745 1000\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Ginn01a 51 27\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Hu19 12 0\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Kars90 26 7\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Kars91 28 15\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Kars96b 91 39\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Litj17 3681 1000\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Murp10a 45 28\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Niem09c 74 41\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Staa04a 2720 1000\n",
      "adding gscites Tell21 154\n",
      "adding gscites Teuw18 0\n",
      "adding gscites Thag23 2\n",
      "adding gscites Thee20 0\n",
      "adding gscites Thij23 0\n",
      "updating Timp02 from 10 to 11\n",
      "updating Timp10 from 24 to 35\n",
      "updating Trom12 from 3 to 4\n",
      "adding gscites Turn21 4\n",
      "adding gscites Valk19a 16\n",
      "updating Vare05 from 46 to 52\n",
      "updating Veli08d from 4 to 6\n",
      "warning: num citations calculated for this bibkey is much lower than previously suggested.... Veli09 62 1\n",
      "updating Veli09a from 2 to 3\n",
      "updating Veli12 from 30 to 34\n",
      "updating Veli13 from 36 to 45\n",
      "updating Velz20 from 10 to 121\n",
      "updating Ven11a from 8 to 12\n",
      "updating Ven13b from 40 to 45\n",
      "updating Ven16a from 15 to 21\n",
      "updating Ven16f from 6 to 7\n",
      "updating Vend17c from 18 to 36\n",
      "adding gscites Vend18 54\n",
      "adding gscites Venh15a 0\n",
      "updating Venh15b from 45 to 72\n",
      "adding gscites Venh15c 13\n",
      "adding gscites Venh16a 11\n",
      "updating Venh17a from 37 to 89\n",
      "updating Venh17b from 59 to 112\n",
      "adding gscites Venh18 112\n",
      "adding gscites Venk21 51\n",
      "adding gscites Venk23 1\n",
      "adding gscites Vent20 2\n",
      "adding gscites Vent21 16\n",
      "adding gscites Vent23 6\n",
      "updating Veta18 from 56 to 205\n",
      "adding gscites Vina22 3\n",
      "adding gscites Vink88 19\n",
      "adding gscites Vlie22 12\n",
      "updating Voge05 from 5 to 6\n",
      "updating Voge07 from 43 to 45\n",
      "updating Vos08 from 103 to 104\n",
      "updating Vos09 from 14 to 15\n",
      "updating Vos13 from 150 to 162\n",
      "adding gscites Vos19 72\n",
      "adding gscites Vos21 5\n",
      "updating Vree17 from 21 to 39\n",
      "updating Vree18 from 11 to 17\n",
      "updating Vree18b from 14 to 24\n",
      "updating Vree18c from 14 to 34\n",
      "updating Vree18d from 25 to 44\n",
      "adding gscites Vug18 0\n",
      "updating Vuka11 from 4 to 6\n",
      "updating Vuka12 from 24 to 28\n",
      "updating Waal15 from 13 to 17\n",
      "updating Wand17 from 57 to 102\n",
      "updating Wand17a from 28 to 54\n",
      "adding gscites Wild21 1\n",
      "adding gscites Wild23a 1\n",
      "adding gscites Wild23b 1\n",
      "adding gscites Wild23c 0\n",
      "updating Wink15a from 51 to 65\n",
      "adding gscites Wink21 18\n",
      "updating Witt11 from 21 to 24\n",
      "updating Witt12 from 31 to 41\n",
      "adding gscites Witt12a 17\n",
      "updating Witt12b from 11 to 14\n",
      "updating Xie20 from 5 to 91\n",
      "adding gscites Xie21 0\n",
      "adding gscites Xie23 2\n",
      "adding gscites Xie23b 0\n",
      "adding gscites Yu20 24\n",
      "updating Zaid18 from 15 to 45\n",
      "updating Zels15 from 19 to 34\n",
      "updating Zels17b from 32 to 51\n",
      "updating Zels18 from 23 to 51\n",
      "updating Zels18a from 22 to 44\n",
      "adding gscites Zels19a 14\n",
      "adding gscites Zhou20 334\n",
      "done updating citations\n"
     ]
    }
   ],
   "source": [
    "diag_bib_raw_new_cits = update_citation_count(diag_bib_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: update to the correct output path\n",
    "save_to_file(diag_bib_raw_new_cits, None, diag_bib_path_tmp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Update the blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39 items added to blacklist'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last we update the blacklist (temporarily commented) (what failures can happen here?)\n",
    "blacklist_df = pd.read_csv('./script_data/blacklist.csv')\n",
    "# TODO: fix to correct output location\n",
    "blacklist_out_file = './script_data/blacklist_tmp_updated.csv'\n",
    "# file writing\n",
    "update_blacklist_csv(blacklist_df, blacklist_items, blacklist_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE with processing manually checked items\n",
      "Failures are as follows:\n",
      "Failed to add new bib entry  03ad8d7078805db6fbd4993b881045b462b4e028\n",
      "Failed to add new bib entry  06ee6ed85131848ef70da625806ba480915aa2e0\n",
      "Failed to add new bib entry  0b78520bea8310ff375cf953bbde10082db0eede\n",
      "Failed to add new bib entry  0df7a4f26d57eb58fe628316aa5e84e5ca474ee8\n",
      "Failed to add new bib entry  0ebe8ab65571514718283cd2d8ac7277db3513c5\n",
      "Failed to add new bib entry  0f27fc10d593859a440c6ccf901d5093f67939bd\n",
      "Failed to add new bib entry  17b918178a85cdb670be7521e6cef3b4dbffb16b\n",
      "Failed to add new bib entry  1d2109f8ec43c23db647c4778a5bb5846074e575\n",
      "Failed to add new bib entry  202f393ad41b85acbc59a28e5080d19c9de56988\n",
      "Failed to add new bib entry  233a8c1b929ccbb0f4a31720919e2b9f413a239c\n",
      "Failed to add new bib entry  269e8609dff88d78e8e3c41f81a97199c9add3dd\n",
      "Failed to add new bib entry  2f2182f8e55be5a85c1316cd1b181cd5c85c106c\n",
      "Failed to add new bib entry  32af51ced47419cff26fde66cce602fbab2f238a\n",
      "Failed to add new bib entry  362c510dec0d566d22d5be3af0519fc7eec8bb86\n",
      "Failed to add new bib entry  36fb5c86a92b941f0754cb864e2c4c70b21b7b7d\n",
      "Failed to add new bib entry  5a09637587e694a03f68a4ee1046d31aa97fd0c0\n",
      "Failed to add new bib entry  5cf666b6326b85a31b4e2759031392f0a49351b2\n",
      "Failed to add new bib entry  663336d2c2efa0a4bfe8a4988eb8d1b87e9a7403\n",
      "Failed to add new bib entry  6769e24a1e5a4b5841fd8bcb0b8daa3051b52214\n",
      "Failed to add new bib entry  67f07af40a5c7e2b008509e4e8f61030ce9f85ab\n",
      "Failed to add new bib entry  7593ee0a8242026714b37ba2c2805d1249c82e13\n",
      "Failed to add new bib entry  8706660fbf3110338bc794e354bc3d1c0075d230\n",
      "Failed to add new bib entry  877ce11291735ee26d2a618adde8db726808b107\n",
      "Failed to add new bib entry  926fe98caca0db9cef3d065f90b19ace24dedc76\n",
      "Failed to add new bib entry  952bdfe8a2732d537363028114718edad19bc451\n",
      "Failed to add new bib entry  98cec4020ccc1ef0399b4f866544a30fb550d34c\n",
      "Failed to add new bib entry  99965d464a74d2083ce198156e6b0ca4d043b128\n",
      "Failed to add new bib entry  9ddb2f47695191553a3623ac33eddeb9c7e416cd\n",
      "Failed to add new bib entry  a0f7cee93c06634d1945b614463977548c2b94bd\n",
      "Failed to add new bib entry  af346d53f267840fc87db7ff4f1ff1b97cfe713a\n",
      "Failed to add new bib entry  b8c484519a8970bf4cb07c3c609c41a05365f258\n",
      "Failed to add new bib entry  be95c8bbd8a4297d620b1c2644cf2a898603e355\n",
      "Failed to add new bib entry  c0f6940d1af8063139d99b12f7e451169278ec33\n",
      "Failed to add new bib entry  c705e376f3d2aaf91deb1ca806e838d94a38dda8\n",
      "Failed to add new bib entry  cf46e880665be3dd1b2a81cc1be53f1ea9d64de1\n",
      "Failed to add new bib entry  ddd8894c4281d91727ab1827501cde67f5cc322d\n",
      "Failed to add new bib entry  e1d86927b130950ef8c67caa76364bb336b083d7\n",
      "Failed to add new bib entry  e4729ac7bfdb707e3207b0a91b57a2f907f5351b\n",
      "Failed to add new bib entry  e4dbe4f2c08eeae4cab346e24327eefa65e44191\n",
      "Failed to add new bib entry  e717ffb38990d5da64a82f8b8715bfb56daf9762\n",
      "Failed to add new bib entry  ea9cff43d07c6e1e63dc9d88ff13c8ab7e5380af\n",
      "Failed to add new bib entry  eb6e86cf9697391771d1e2fbad3f49c1448e3411\n",
      "Failed to add new bib entry  efaa4e593dbf76786a33dfd2dff79b77396397cc\n",
      "Failed to add new bib entry  f9166fa0c5c102618d890bdeab63aca74b017c45\n",
      "Failed to find valid action for item 03ef312b3d3e616fd7f0a2f2260c82ad62ed7ef1 [add ss_id, blacklist ss_id, add new item]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Here we provide a report of rows where we did not know what to do or we failed to do the action\n",
    "print(\"DONE with processing manually checked items\")\n",
    "print('Failures are as follows:')\n",
    "for item in failed_new_items:\n",
    "    print('Failed to add new bib entry ', item['ss_id'])\n",
    "for item in failed_updated_items:\n",
    "    print('Failed to update exiting bib entry with new ss_id', item['bibkey'], item['ss_id'])\n",
    "for item in failed_to_find_actions:\n",
    "    print('Failed to find valid action for item', item['ss_id'], item['action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newly added items: 173\n"
     ]
    }
   ],
   "source": [
    "count = items_to_add.count('{yes}')\n",
    "print(f\"Newly added items: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Blacklisted items: {len(blacklist_items)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
